# ðŸ“š NLP Agent - Documentation ComplÃ¨te

## ðŸŽ¯ **Vue d'Ensemble du Projet**

### **Qu'est-ce que NLP Agent?**
NLP Agent est une **application web complÃ¨te** dÃ©veloppÃ©e avec Streamlit qui offre **5 fonctionnalitÃ©s d'analyse de texte** avancÃ©es. C'est un projet parfait pour dÃ©montrer vos compÃ©tences en **Data Science**, **NLP**, et **dÃ©veloppement web**.

### **Technologies UtilisÃ©es**
- **Frontend**: Streamlit (interface web interactive)
- **Backend**: Python avec bibliothÃ¨ques NLP
- **Machine Learning**: scikit-learn, NLTK, TextBlob
- **Visualisation**: Plotly, Matplotlib
- **DÃ©ploiement**: Streamlit Cloud + GitHub

---

## ðŸ—ï¸ **Architecture du Projet**

### **Structure des Fichiers**
```
NLP-Agent/
â”œâ”€â”€ ðŸ“„ app.py                    # Application principale Streamlit
â”œâ”€â”€ ðŸ“ src/                      # Modules Python
â”‚   â”œâ”€â”€ data_preprocessing.py    # PrÃ©processing de texte
â”‚   â”œâ”€â”€ sentiment_analysis.py    # Analyse de sentiment
â”‚   â””â”€â”€ simple_data_preprocessing.py  # Version simplifiÃ©e
â”œâ”€â”€ ðŸ“ models/                   # ModÃ¨les prÃ©-entraÃ®nÃ©s
â”‚   â”œâ”€â”€ lda_model.pkl           # ModÃ¨le LDA pour topic modeling
â”‚   â””â”€â”€ tfidf_vectorizer.pkl    # Vectoriseur TF-IDF
â”œâ”€â”€ ðŸ“„ requirements.txt         # DÃ©pendances Python
â”œâ”€â”€ ðŸ“„ .gitignore              # Fichiers Ã  ignorer par Git
â””â”€â”€ ðŸ“„ README.md               # Documentation du projet
```

### **Flux de DonnÃ©es**
```
Texte d'entrÃ©e â†’ PrÃ©processing â†’ Analyse â†’ Visualisation â†’ RÃ©sultats
```

---

## ðŸ”§ **FonctionnalitÃ©s DÃ©taillÃ©es**

### **1. ðŸ“Š Data Exploration (Exploration de DonnÃ©es)**

#### **Objectif**
Analyser les caractÃ©ristiques statistiques et structurelles du texte.

#### **FonctionnalitÃ©s**
- **Statistiques de base**: Nombre de mots, phrases, caractÃ¨res
- **Analyse de frÃ©quence**: Mots les plus frÃ©quents
- **Visualisations**: Graphiques interactifs avec Plotly
- **Nuage de mots**: ReprÃ©sentation visuelle des mots importants

#### **Code Principal**
```python
def show_data_exploration(text, modules):
    # Calcul des statistiques
    words = text.split()
    sentences = text.count('.') + text.count('!') + text.count('?')
    
    # Visualisation avec Plotly
    word_freq = pd.Series(words).value_counts().head(10)
    fig = px.bar(x=word_freq.values, y=word_freq.index)
    st.plotly_chart(fig)
```

### **2. ðŸ˜Š Sentiment Analysis (Analyse de Sentiment)**

#### **Objectif**
DÃ©terminer l'Ã©motion (positive, nÃ©gative, neutre) exprimÃ©e dans le texte.

#### **MÃ©thodes UtilisÃ©es**
1. **TextBlob**: Analyse rapide basÃ©e sur des lexiques
2. **Patterns**: Analyse basÃ©e sur des rÃ¨gles linguistiques
3. **Machine Learning**: Classification avec scikit-learn

#### **Code Principal**
```python
def analyze_sentiment(text):
    # TextBlob
    blob = TextBlob(text)
    polarity = blob.sentiment.polarity
    
    # Classification
    if polarity > 0.1:
        return 'positive'
    elif polarity < -0.1:
        return 'negative'
    else:
        return 'neutral'
```

### **3. ðŸŽ¯ Topic Modeling (ModÃ©lisation de Sujets)**

#### **Objectif**
Identifier automatiquement les thÃ¨mes principaux dans le texte.

#### **Algorithme: LDA (Latent Dirichlet Allocation)**
- **Principe**: Chaque document est un mÃ©lange de sujets
- **Sortie**: Distribution de probabilitÃ© sur les sujets
- **Visualisation**: Graphiques de distribution des sujets

#### **Code Principal**
```python
# EntraÃ®nement du modÃ¨le LDA
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(max_features=100)
tfidf_matrix = vectorizer.fit_transform([text])

lda = LatentDirichletAllocation(n_components=3)
lda.fit(tfidf_matrix)
```

### **4. ðŸ·ï¸ Named Entity Recognition (Reconnaissance d'EntitÃ©s)**

#### **Objectif**
Identifier et classifier les entitÃ©s nommÃ©es (personnes, lieux, organisations).

#### **MÃ©thodes**
1. **spaCy** (si disponible): ModÃ¨le prÃ©-entraÃ®nÃ©
2. **Patterns regex** (fallback): Reconnaissance basÃ©e sur des motifs

#### **Types d'EntitÃ©s DÃ©tectÃ©es**
- **PERSON**: Noms de personnes
- **ORG**: Organisations, entreprises
- **GPE**: Lieux gÃ©opolitiques (villes, pays)
- **DATE**: Dates et expressions temporelles

### **5. ðŸ“ Text Summarization (RÃ©sumÃ© de Texte)**

#### **Objectif**
CrÃ©er automatiquement un rÃ©sumÃ© concis du texte original.

#### **MÃ©thodes ImplÃ©mentÃ©es**
1. **TF-IDF**: SÃ©lection des phrases les plus importantes
2. **FrÃ©quence**: BasÃ© sur la frÃ©quence des mots
3. **Position**: PrivilÃ©gie les premiÃ¨res phrases

#### **Code Principal**
```python
def tfidf_based_summary(text, num_sentences=3):
    sentences = sent_tokenize(text)
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(sentences)
    
    # Calcul des scores
    scores = tfidf_matrix.sum(axis=1).A1
    ranked_sentences = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)
    
    # SÃ©lection des meilleures phrases
    summary_sentences = [sentences[i] for i, _ in ranked_sentences[:num_sentences]]
    return ' '.join(summary_sentences)
```

---

## ðŸ› ï¸ **Installation et Configuration**

### **Ã‰tape 1: Cloner le Projet**
```bash
git clone https://github.com/yassinebenacha/NLP-Agent.git
cd NLP-Agent
```

### **Ã‰tape 2: CrÃ©er un Environnement Virtuel**
```bash
# CrÃ©er l'environnement
python -m venv nlp_env

# Activer l'environnement
# Windows:
nlp_env\Scripts\activate
# Mac/Linux:
source nlp_env/bin/activate
```

### **Ã‰tape 3: Installer les DÃ©pendances**
```bash
pip install -r requirements.txt
```

### **Ã‰tape 4: TÃ©lÃ©charger les DonnÃ©es NLTK**
```python
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
```

### **Ã‰tape 5: Lancer l'Application**
```bash
streamlit run app.py
```

---

## ðŸ“¦ **DÃ©pendances ExpliquÃ©es**

### **requirements.txt**
```
streamlit>=1.28.0      # Framework web pour l'interface
pandas>=1.5.0          # Manipulation de donnÃ©es
numpy>=1.21.0          # Calculs numÃ©riques
matplotlib>=3.5.0      # Visualisations statiques
plotly>=5.0.0          # Visualisations interactives
scikit-learn>=1.1.0    # Machine learning
nltk>=3.8              # Traitement du langage naturel
textblob>=0.17.0       # Analyse de sentiment simple
```

### **Pourquoi ces BibliothÃ¨ques?**
- **Streamlit**: Interface web sans JavaScript
- **Pandas**: Manipulation efficace des donnÃ©es textuelles
- **Plotly**: Graphiques interactifs pour l'exploration
- **scikit-learn**: Algorithmes ML robustes et optimisÃ©s
- **NLTK**: Outils NLP complets et bien documentÃ©s

---

## ðŸŽ¨ **Interface Utilisateur**

### **Navigation Sidebar**
```python
# SÃ©lection de la fonctionnalitÃ©
analysis_type = st.sidebar.selectbox(
    "Choose Analysis Tool:",
    ["ðŸ  Home", "ðŸ“Š Data Exploration", "ðŸ˜Š Sentiment Analysis", 
     "ðŸŽ¯ Topic Modeling", "ðŸ·ï¸ Named Entity Recognition", 
     "ðŸ“ Text Summarization"]
)
```

### **Zone de Saisie de Texte**
```python
# Options d'entrÃ©e multiples
input_method = st.radio("Choose input method:", 
                       ["âœï¸ Type text", "ðŸ“ Upload file", "ðŸ“‹ Use sample"])

if input_method == "âœï¸ Type text":
    text = st.text_area("Enter your text:", height=200)
elif input_method == "ðŸ“ Upload file":
    uploaded_file = st.file_uploader("Choose a file", type=['txt', 'csv'])
```

### **Affichage des RÃ©sultats**
```python
# Colonnes pour organiser l'affichage
col1, col2 = st.columns(2)

with col1:
    st.metric("Word Count", len(words))
    
with col2:
    st.metric("Sentence Count", num_sentences)
```

---

## ðŸ§  **Algorithmes et Concepts NLP**

### **1. PrÃ©processing de Texte**

#### **Ã‰tapes du PrÃ©processing**
```python
def preprocess_text(text):
    # 1. Nettoyage de base
    text = re.sub(r'http\S+', '', text)  # Supprimer URLs
    text = re.sub(r'[^\w\s]', '', text)  # Supprimer ponctuation

    # 2. Normalisation
    text = text.lower()  # Minuscules

    # 3. Tokenisation
    tokens = word_tokenize(text)

    # 4. Suppression des mots vides
    stop_words = set(stopwords.words('english'))
    tokens = [t for t in tokens if t not in stop_words]

    # 5. Lemmatisation
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(t) for t in tokens]

    return ' '.join(tokens)
```

#### **Pourquoi PrÃ©processer?**
- **RÃ©duction du bruit**: Ã‰liminer les Ã©lÃ©ments non informatifs
- **Normalisation**: Uniformiser le format du texte
- **Optimisation**: RÃ©duire la dimensionnalitÃ© des donnÃ©es

### **2. TF-IDF (Term Frequency-Inverse Document Frequency)**

#### **Formule MathÃ©matique**
```
TF-IDF(t,d) = TF(t,d) Ã— IDF(t)

oÃ¹:
TF(t,d) = (Nombre d'occurrences de t dans d) / (Nombre total de mots dans d)
IDF(t) = log(Nombre total de documents / Nombre de documents contenant t)
```

#### **ImplÃ©mentation**
```python
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(
    max_features=1000,      # Limiter le vocabulaire
    ngram_range=(1, 2),     # Unigrammes et bigrammes
    stop_words='english'    # Supprimer mots vides
)

tfidf_matrix = vectorizer.fit_transform(documents)
```

### **3. LDA (Latent Dirichlet Allocation)**

#### **Principe**
- Chaque **document** est un mÃ©lange de **sujets**
- Chaque **sujet** est un mÃ©lange de **mots**
- Algorithme probabiliste pour dÃ©couvrir ces mÃ©langes

#### **ParamÃ¨tres Importants**
```python
lda = LatentDirichletAllocation(
    n_components=5,         # Nombre de sujets
    random_state=42,        # ReproductibilitÃ©
    max_iter=100,          # Nombre d'itÃ©rations
    learning_method='batch' # MÃ©thode d'apprentissage
)
```

### **4. Analyse de Sentiment avec TextBlob**

#### **MÃ©triques**
- **Polarity**: [-1, 1] (nÃ©gatif â†’ positif)
- **Subjectivity**: [0, 1] (objectif â†’ subjectif)

#### **Classification**
```python
def classify_sentiment(polarity):
    if polarity > 0.1:
        return "Positive ðŸ˜Š"
    elif polarity < -0.1:
        return "Negative ðŸ˜ž"
    else:
        return "Neutral ðŸ˜"
```

---

## ðŸš€ **DÃ©ploiement sur Streamlit Cloud**

### **Ã‰tape 1: PrÃ©parer le Repository GitHub**
```bash
# VÃ©rifier que tous les fichiers sont inclus
git add .
git commit -m "Prepare for deployment"
git push origin main
```

### **Ã‰tape 2: Configurer Streamlit Cloud**
1. Aller sur [share.streamlit.io](https://share.streamlit.io)
2. Se connecter avec GitHub
3. SÃ©lectionner le repository `NLP-Agent`
4. Choisir `app.py` comme fichier principal
5. Cliquer sur "Deploy"

### **Ã‰tape 3: VÃ©rifier le DÃ©ploiement**
L'application devrait afficher:
- âœ… **LDA model loaded**
- âœ… **TF-IDF vectorizer loaded**
- âœ… **Text Preprocessor loaded**
- âœ… **Sentiment Analyzer loaded**
- âš ï¸ **spaCy NER not available** (normal)

---

## ðŸ”§ **Gestion des Erreurs et Fallbacks**

### **StratÃ©gie de Robustesse**
```python
# Exemple de fallback pour spaCy
try:
    import spacy
    nlp = spacy.load("en_core_web_sm")
    SPACY_AVAILABLE = True
except:
    SPACY_AVAILABLE = False
    # Utiliser des patterns regex comme fallback
```

### **Gestion des DÃ©pendances Optionnelles**
```python
def safe_import(module_name, fallback_func=None):
    try:
        return __import__(module_name)
    except ImportError:
        if fallback_func:
            return fallback_func()
        return None
```

---

## ðŸ“Š **MÃ©triques et Ã‰valuation**

### **MÃ©triques de Performance**
- **Temps de traitement**: Mesure de la vitesse d'analyse
- **PrÃ©cision**: QualitÃ© des rÃ©sultats (pour sentiment)
- **Couverture**: Pourcentage d'entitÃ©s dÃ©tectÃ©es

### **Validation des RÃ©sultats**
```python
def validate_sentiment_analysis(predictions, ground_truth):
    from sklearn.metrics import accuracy_score, classification_report

    accuracy = accuracy_score(ground_truth, predictions)
    report = classification_report(ground_truth, predictions)

    return accuracy, report
```

---

## ðŸŽ¯ **Cas d'Usage et Applications**

### **1. Analyse de Feedback Client**
- Analyser les avis produits
- Identifier les points d'amÃ©lioration
- Mesurer la satisfaction client

### **2. Veille MÃ©diatique**
- Surveiller les mentions de marque
- Analyser le sentiment des articles
- Identifier les sujets tendance

### **3. Analyse de Contenu**
- RÃ©sumer des documents longs
- Extraire les entitÃ©s importantes
- Classifier le contenu par thÃ¨me

### **4. Recherche AcadÃ©mique**
- Analyser des corpus de textes
- Identifier les thÃ¨mes de recherche
- Extraire des insights quantitatifs

---

## ðŸ† **Avantages pour votre Profil**

### **CompÃ©tences DÃ©montrÃ©es**
- **Data Science**: Preprocessing, ML, Ã©valuation
- **NLP**: Sentiment, NER, topic modeling, summarization
- **DÃ©veloppement Web**: Interface utilisateur interactive
- **DevOps**: DÃ©ploiement cloud, gestion des dÃ©pendances
- **Gestion de Projet**: Structure, documentation, versioning

### **Points Forts du Projet**
- **Complet**: 5 fonctionnalitÃ©s NLP diffÃ©rentes
- **Robuste**: Gestion d'erreurs et fallbacks
- **Professionnel**: Interface propre et intuitive
- **DÃ©ployÃ©**: Application web accessible publiquement
- **DocumentÃ©**: Code commentÃ© et documentation complÃ¨te

---

## ðŸ“š **Ressources pour Approfondir**

### **Livres RecommandÃ©s**
- "Natural Language Processing with Python" (NLTK Book)
- "Speech and Language Processing" (Jurafsky & Martin)
- "Hands-On Machine Learning" (AurÃ©lien GÃ©ron)

### **Cours en Ligne**
- CS224N (Stanford NLP Course)
- Fast.ai NLP Course
- Coursera NLP Specialization

### **Documentation Officielle**
- [Streamlit Docs](https://docs.streamlit.io)
- [scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html)
- [NLTK Documentation](https://www.nltk.org)

---

## ðŸŽ‰ **Conclusion**

Votre **NLP Agent** est un projet complet qui dÃ©montre une maÃ®trise solide des technologies modernes de Data Science et NLP. Il combine:

- **ThÃ©orie**: Algorithmes NLP avancÃ©s
- **Pratique**: ImplÃ©mentation robuste
- **DÃ©ploiement**: Application web fonctionnelle
- **Documentation**: Guide complet et professionnel

**C'est un excellent atout pour vos candidatures de stage et votre portfolio professionnel! ðŸš€**
