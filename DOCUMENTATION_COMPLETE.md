# üìö NLP Agent - Documentation Compl√®te

## üéØ **Vue d'Ensemble du Projet**

### **Qu'est-ce que NLP Agent?**
NLP Agent est une **application web compl√®te** d√©velopp√©e avec Streamlit qui offre **5 fonctionnalit√©s d'analyse de texte** avanc√©es. C'est un projet parfait pour d√©montrer vos comp√©tences en **Data Science**, **NLP**, et **d√©veloppement web**.

### **Technologies Utilis√©es**
- **Frontend**: Streamlit (interface web interactive)
- **Backend**: Python avec biblioth√®ques NLP
- **Machine Learning**: scikit-learn, NLTK, TextBlob
- **Visualisation**: Plotly, Matplotlib
- **D√©ploiement**: Streamlit Cloud + GitHub

---

## üèóÔ∏è **Architecture du Projet**

### **Structure des Fichiers**
```
NLP-Agent/
‚îú‚îÄ‚îÄ üìÑ app.py                    # Application principale Streamlit
‚îú‚îÄ‚îÄ üìÅ src/                      # Modules Python
‚îÇ   ‚îú‚îÄ‚îÄ data_preprocessing.py    # Pr√©processing de texte
‚îÇ   ‚îú‚îÄ‚îÄ sentiment_analysis.py    # Analyse de sentiment
‚îÇ   ‚îî‚îÄ‚îÄ simple_data_preprocessing.py  # Version simplifi√©e
‚îú‚îÄ‚îÄ üìÅ models/                   # Mod√®les pr√©-entra√Æn√©s
‚îÇ   ‚îú‚îÄ‚îÄ lda_model.pkl           # Mod√®le LDA pour topic modeling
‚îÇ   ‚îî‚îÄ‚îÄ tfidf_vectorizer.pkl    # Vectoriseur TF-IDF
‚îú‚îÄ‚îÄ üìÑ requirements.txt         # D√©pendances Python
‚îú‚îÄ‚îÄ üìÑ .gitignore              # Fichiers √† ignorer par Git
‚îî‚îÄ‚îÄ üìÑ README.md               # Documentation du projet
```

### **Flux de Donn√©es**
```
Texte d'entr√©e ‚Üí Pr√©processing ‚Üí Analyse ‚Üí Visualisation ‚Üí R√©sultats
```

---

## üîß **Fonctionnalit√©s D√©taill√©es**

### **1. üìä Data Exploration (Exploration de Donn√©es)**

#### **Objectif**
Analyser les caract√©ristiques statistiques et structurelles du texte.

#### **Fonctionnalit√©s**
- **Statistiques de base**: Nombre de mots, phrases, caract√®res
- **Analyse de fr√©quence**: Mots les plus fr√©quents
- **Visualisations**: Graphiques interactifs avec Plotly
- **Nuage de mots**: Repr√©sentation visuelle des mots importants

#### **Code Principal**
```python
def show_data_exploration(text, modules):
    # Calcul des statistiques
    words = text.split()
    sentences = text.count('.') + text.count('!') + text.count('?')
    
    # Visualisation avec Plotly
    word_freq = pd.Series(words).value_counts().head(10)
    fig = px.bar(x=word_freq.values, y=word_freq.index)
    st.plotly_chart(fig)
```

### **2. üòä Sentiment Analysis (Analyse de Sentiment)**

#### **Objectif**
D√©terminer l'√©motion (positive, n√©gative, neutre) exprim√©e dans le texte.

#### **M√©thodes Utilis√©es**
1. **TextBlob**: Analyse rapide bas√©e sur des lexiques
2. **Patterns**: Analyse bas√©e sur des r√®gles linguistiques
3. **Machine Learning**: Classification avec scikit-learn

#### **Code Principal**
```python
def analyze_sentiment(text):
    # TextBlob
    blob = TextBlob(text)
    polarity = blob.sentiment.polarity
    
    # Classification
    if polarity > 0.1:
        return 'positive'
    elif polarity < -0.1:
        return 'negative'
    else:
        return 'neutral'
```

### **3. üéØ Topic Modeling (Mod√©lisation de Sujets)**

#### **Objectif**
Identifier automatiquement les th√®mes principaux dans le texte.

#### **Algorithme: LDA (Latent Dirichlet Allocation)**
- **Principe**: Chaque document est un m√©lange de sujets
- **Sortie**: Distribution de probabilit√© sur les sujets
- **Visualisation**: Graphiques de distribution des sujets

#### **Code Principal**
```python
# Entra√Ænement du mod√®le LDA
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(max_features=100)
tfidf_matrix = vectorizer.fit_transform([text])

lda = LatentDirichletAllocation(n_components=3)
lda.fit(tfidf_matrix)
```

### **4. üè∑Ô∏è Named Entity Recognition (Reconnaissance d'Entit√©s)**

#### **Objectif**
Identifier et classifier les entit√©s nomm√©es (personnes, lieux, organisations).

#### **M√©thodes**
1. **spaCy** (si disponible): Mod√®le pr√©-entra√Æn√©
2. **Patterns regex** (fallback): Reconnaissance bas√©e sur des motifs

#### **Types d'Entit√©s D√©tect√©es**
- **PERSON**: Noms de personnes
- **ORG**: Organisations, entreprises
- **GPE**: Lieux g√©opolitiques (villes, pays)
- **DATE**: Dates et expressions temporelles

### **5. üìù Text Summarization (R√©sum√© de Texte)**

#### **Objectif**
Cr√©er automatiquement un r√©sum√© concis du texte original.

#### **M√©thodes Impl√©ment√©es**
1. **TF-IDF**: S√©lection des phrases les plus importantes
2. **Fr√©quence**: Bas√© sur la fr√©quence des mots
3. **Position**: Privil√©gie les premi√®res phrases

#### **Code Principal**
```python
def tfidf_based_summary(text, num_sentences=3):
    sentences = sent_tokenize(text)
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(sentences)
    
    # Calcul des scores
    scores = tfidf_matrix.sum(axis=1).A1
    ranked_sentences = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)
    
    # S√©lection des meilleures phrases
    summary_sentences = [sentences[i] for i, _ in ranked_sentences[:num_sentences]]
    return ' '.join(summary_sentences)
```

---

## üõ†Ô∏è **Installation et Configuration**

### **√âtape 1: Cloner le Projet**
```bash
git clone https://github.com/yassinebenacha/NLP-Agent.git
cd NLP-Agent
```

### **√âtape 2: Cr√©er un Environnement Virtuel**
```bash
# Cr√©er l'environnement
python -m venv nlp_env

# Activer l'environnement
# Windows:
nlp_env\Scripts\activate
# Mac/Linux:
source nlp_env/bin/activate
```

### **√âtape 3: Installer les D√©pendances**
```bash
pip install -r requirements.txt
```

### **√âtape 4: T√©l√©charger les Donn√©es NLTK**
```python
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
```

### **√âtape 5: Lancer l'Application**
```bash
streamlit run app.py
```

---

## üì¶ **D√©pendances Expliqu√©es**

### **requirements.txt**
```
streamlit>=1.28.0      # Framework web pour l'interface
pandas>=1.5.0          # Manipulation de donn√©es
numpy>=1.21.0          # Calculs num√©riques
matplotlib>=3.5.0      # Visualisations statiques
plotly>=5.0.0          # Visualisations interactives
scikit-learn>=1.1.0    # Machine learning
nltk>=3.8              # Traitement du langage naturel
textblob>=0.17.0       # Analyse de sentiment simple
```

### **Pourquoi ces Biblioth√®ques?**
- **Streamlit**: Interface web sans JavaScript
- **Pandas**: Manipulation efficace des donn√©es textuelles
- **Plotly**: Graphiques interactifs pour l'exploration
- **scikit-learn**: Algorithmes ML robustes et optimis√©s
- **NLTK**: Outils NLP complets et bien document√©s

---

## üé® **Interface Utilisateur**

### **Navigation Sidebar**
```python
# S√©lection de la fonctionnalit√©
analysis_type = st.sidebar.selectbox(
    "Choose Analysis Tool:",
    ["üè† Home", "üìä Data Exploration", "üòä Sentiment Analysis", 
     "üéØ Topic Modeling", "üè∑Ô∏è Named Entity Recognition", 
     "üìù Text Summarization"]
)
```

### **Zone de Saisie de Texte**
```python
# Options d'entr√©e multiples
input_method = st.radio("Choose input method:", 
                       ["‚úçÔ∏è Type text", "üìÅ Upload file", "üìã Use sample"])

if input_method == "‚úçÔ∏è Type text":
    text = st.text_area("Enter your text:", height=200)
elif input_method == "üìÅ Upload file":
    uploaded_file = st.file_uploader("Choose a file", type=['txt', 'csv'])
```

### **Affichage des R√©sultats**
```python
# Colonnes pour organiser l'affichage
col1, col2 = st.columns(2)

with col1:
    st.metric("Word Count", len(words))
    
with col2:
    st.metric("Sentence Count", num_sentences)
```

---

## üß† **Algorithmes et Concepts NLP**

### **1. Pr√©processing de Texte**

#### **√âtapes du Pr√©processing**
```python
def preprocess_text(text):
    # 1. Nettoyage de base
    text = re.sub(r'http\S+', '', text)  # Supprimer URLs
    text = re.sub(r'[^\w\s]', '', text)  # Supprimer ponctuation

    # 2. Normalisation
    text = text.lower()  # Minuscules

    # 3. Tokenisation
    tokens = word_tokenize(text)

    # 4. Suppression des mots vides
    stop_words = set(stopwords.words('english'))
    tokens = [t for t in tokens if t not in stop_words]

    # 5. Lemmatisation
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(t) for t in tokens]

    return ' '.join(tokens)
```

#### **Pourquoi Pr√©processer?**
- **R√©duction du bruit**: √âliminer les √©l√©ments non informatifs
- **Normalisation**: Uniformiser le format du texte
- **Optimisation**: R√©duire la dimensionnalit√© des donn√©es

### **2. TF-IDF (Term Frequency-Inverse Document Frequency)**

#### **Formule Math√©matique**
```
TF-IDF(t,d) = TF(t,d) √ó IDF(t)

o√π:
TF(t,d) = (Nombre d'occurrences de t dans d) / (Nombre total de mots dans d)
IDF(t) = log(Nombre total de documents / Nombre de documents contenant t)
```

#### **Impl√©mentation**
```python
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(
    max_features=1000,      # Limiter le vocabulaire
    ngram_range=(1, 2),     # Unigrammes et bigrammes
    stop_words='english'    # Supprimer mots vides
)

tfidf_matrix = vectorizer.fit_transform(documents)
```

### **3. LDA (Latent Dirichlet Allocation)**

#### **Principe**
- Chaque **document** est un m√©lange de **sujets**
- Chaque **sujet** est un m√©lange de **mots**
- Algorithme probabiliste pour d√©couvrir ces m√©langes

#### **Param√®tres Importants**
```python
lda = LatentDirichletAllocation(
    n_components=5,         # Nombre de sujets
    random_state=42,        # Reproductibilit√©
    max_iter=100,          # Nombre d'it√©rations
    learning_method='batch' # M√©thode d'apprentissage
)
```

### **4. Analyse de Sentiment avec TextBlob**

#### **M√©triques**
- **Polarity**: [-1, 1] (n√©gatif ‚Üí positif)
- **Subjectivity**: [0, 1] (objectif ‚Üí subjectif)

#### **Classification**
```python
def classify_sentiment(polarity):
    if polarity > 0.1:
        return "Positive üòä"
    elif polarity < -0.1:
        return "Negative üòû"
    else:
        return "Neutral üòê"
```

---

## üöÄ **D√©ploiement sur Streamlit Cloud**

### **√âtape 1: Pr√©parer le Repository GitHub**
```bash
# V√©rifier que tous les fichiers sont inclus
git add .
git commit -m "Prepare for deployment"
git push origin main
```

### **√âtape 2: Configurer Streamlit Cloud**
1. Aller sur [share.streamlit.io](https://share.streamlit.io)
2. Se connecter avec GitHub
3. S√©lectionner le repository `NLP-Agent`
4. Choisir `app.py` comme fichier principal
5. Cliquer sur "Deploy"

### **√âtape 3: V√©rifier le D√©ploiement**
L'application devrait afficher:
- ‚úÖ **LDA model loaded**
- ‚úÖ **TF-IDF vectorizer loaded**
- ‚úÖ **Text Preprocessor loaded**
- ‚úÖ **Sentiment Analyzer loaded**
- ‚ö†Ô∏è **spaCy NER not available** (normal)

---

## üîß **Gestion des Erreurs et Fallbacks**

### **Strat√©gie de Robustesse**
```python
# Exemple de fallback pour spaCy
try:
    import spacy
    nlp = spacy.load("en_core_web_sm")
    SPACY_AVAILABLE = True
except:
    SPACY_AVAILABLE = False
    # Utiliser des patterns regex comme fallback
```

### **Gestion des D√©pendances Optionnelles**
```python
def safe_import(module_name, fallback_func=None):
    try:
        return __import__(module_name)
    except ImportError:
        if fallback_func:
            return fallback_func()
        return None
```

---

## üìä **M√©triques et √âvaluation**

### **M√©triques de Performance**
- **Temps de traitement**: Mesure de la vitesse d'analyse
- **Pr√©cision**: Qualit√© des r√©sultats (pour sentiment)
- **Couverture**: Pourcentage d'entit√©s d√©tect√©es

### **Validation des R√©sultats**
```python
def validate_sentiment_analysis(predictions, ground_truth):
    from sklearn.metrics import accuracy_score, classification_report

    accuracy = accuracy_score(ground_truth, predictions)
    report = classification_report(ground_truth, predictions)

    return accuracy, report
```

---

## üéØ **Cas d'Usage et Applications**

### **1. Analyse de Feedback Client**
- Analyser les avis produits
- Identifier les points d'am√©lioration
- Mesurer la satisfaction client

### **2. Veille M√©diatique**
- Surveiller les mentions de marque
- Analyser le sentiment des articles
- Identifier les sujets tendance

### **3. Analyse de Contenu**
- R√©sumer des documents longs
- Extraire les entit√©s importantes
- Classifier le contenu par th√®me

### **4. Recherche Acad√©mique**
- Analyser des corpus de textes
- Identifier les th√®mes de recherche
- Extraire des insights quantitatifs

---

## üèÜ **Avantages pour votre Profil**

### **Comp√©tences D√©montr√©es**
- **Data Science**: Preprocessing, ML, √©valuation
- **NLP**: Sentiment, NER, topic modeling, summarization
- **D√©veloppement Web**: Interface utilisateur interactive
- **DevOps**: D√©ploiement cloud, gestion des d√©pendances
- **Gestion de Projet**: Structure, documentation, versioning

### **Points Forts du Projet**
- **Complet**: 5 fonctionnalit√©s NLP diff√©rentes
- **Robuste**: Gestion d'erreurs et fallbacks
- **Professionnel**: Interface propre et intuitive
- **D√©ploy√©**: Application web accessible publiquement
- **Document√©**: Code comment√© et documentation compl√®te

---

## üìö **Ressources pour Approfondir**

### **Livres Recommand√©s**
- "Natural Language Processing with Python" (NLTK Book)
- "Speech and Language Processing" (Jurafsky & Martin)
- "Hands-On Machine Learning" (Aur√©lien G√©ron)

### **Cours en Ligne**
- CS224N (Stanford NLP Course)
- Fast.ai NLP Course
- Coursera NLP Specialization

### **Documentation Officielle**
- [Streamlit Docs](https://docs.streamlit.io)
- [scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html)
- [NLTK Documentation](https://www.nltk.org)

---

## üéâ **Conclusion**

Votre **NLP Agent** est un projet complet qui d√©montre une ma√Ætrise solide des technologies modernes de Data Science et NLP. Il combine:

- **Th√©orie**: Algorithmes NLP avanc√©s
- **Pratique**: Impl√©mentation robuste
- **D√©ploiement**: Application web fonctionnelle
- **Documentation**: Guide complet et professionnel

**C'est un excellent atout pour vos candidatures de stage et votre portfolio professionnel! üöÄ**
