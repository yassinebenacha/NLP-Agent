# üîß Guide Technique - NLP Agent

## üìã **Table des Mati√®res**
1. [Architecture Technique](#architecture-technique)
2. [Structure du Code](#structure-du-code)
3. [Modules et Classes](#modules-et-classes)
4. [Algorithmes Impl√©ment√©s](#algorithmes-impl√©ment√©s)
5. [Gestion des Erreurs](#gestion-des-erreurs)
6. [Performance et Optimisation](#performance-et-optimisation)
7. [Extension et Personnalisation](#extension-et-personnalisation)

---

## üèóÔ∏è **Architecture Technique**

### **Stack Technologique**
```
Frontend:    Streamlit (Python Web Framework)
Backend:     Python 3.8+
ML/NLP:      scikit-learn, NLTK, TextBlob
Viz:         Plotly, Matplotlib
Deploy:      Streamlit Cloud + GitHub
Storage:     Pickle (mod√®les), Local (temporaire)
```

### **Flux de Donn√©es**
```
Input Text ‚Üí Preprocessing ‚Üí Feature Extraction ‚Üí ML Models ‚Üí Visualization ‚Üí Output
     ‚Üì              ‚Üì              ‚Üì               ‚Üì            ‚Üì           ‚Üì
  Streamlit    TextPreprocessor  TfidfVectorizer   LDA      Plotly     Streamlit
   Widget         Class           sklearn         Model     Charts      Display
```

### **Diagramme d'Architecture**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    STREAMLIT APP (app.py)                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ    Home     ‚îÇ  ‚îÇ Data Explor ‚îÇ  ‚îÇ   Sentiment Anal    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   Page      ‚îÇ  ‚îÇ   ation     ‚îÇ  ‚îÇ      ysis           ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                          ‚îÇ
‚îÇ  ‚îÇ   Topic     ‚îÇ  ‚îÇ    NER      ‚îÇ                          ‚îÇ
‚îÇ  ‚îÇ  Modeling   ‚îÇ  ‚îÇ  Analysis   ‚îÇ                          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                      SRC MODULES                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ data_preprocessing‚îÇ  ‚îÇ    sentiment_analysis.py      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ      .py        ‚îÇ  ‚îÇ                                 ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                     MODELS FOLDER                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  lda_model.pkl  ‚îÇ  ‚îÇ   tfidf_vectorizer.pkl         ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìÅ **Structure du Code**

### **app.py - Application Principale**
```python
# Structure principale
def main():
    setup_page_config()      # Configuration Streamlit
    load_custom_css()        # Styles CSS personnalis√©s
    modules = load_modules() # Chargement des modules NLP
    
    # Navigation sidebar
    analysis_type = st.sidebar.selectbox(...)
    
    # Routage vers les fonctionnalit√©s
    if analysis_type == "üè† Home":
        show_home_page()
    elif analysis_type == "üìä Data Exploration":
        show_data_exploration(text, modules)
    # ... autres fonctionnalit√©s
```

### **Fonctions Principales**
```python
# Chargement des modules
def load_modules() -> Dict[str, Any]:
    """Charge tous les modules NLP avec gestion d'erreurs"""
    
# Pages de fonctionnalit√©s
def show_data_exploration(text: str, modules: Dict) -> None:
def show_sentiment_analysis(text: str, modules: Dict) -> None:
def show_topic_modeling(text: str, models: Dict, modules: Dict) -> None:
def show_named_entity_recognition(text: str, modules: Dict) -> None:
def show_text_summarization(text: str, modules: Dict) -> None:

# Utilitaires
def get_text_input() -> str:
def create_download_link(data: str, filename: str) -> str:
```

---

## üß© **Modules et Classes**

### **1. data_preprocessing.py**

#### **Classe TextPreprocessor**
```python
class TextPreprocessor:
    def __init__(self, 
                 remove_stopwords: bool = True,
                 remove_punctuation: bool = True,
                 lowercase: bool = True,
                 remove_numbers: bool = False,
                 min_word_length: int = 2,
                 max_word_length: int = 50,
                 language: str = 'english'):
        # Initialisation avec fallbacks intelligents
```

#### **M√©thodes Principales**
```python
def clean_text(self, text: str) -> str:
    """Nettoyage de base du texte"""
    # Suppression URLs, emails, HTML tags
    # Normalisation des espaces
    
def preprocess_text(self, text: str) -> str:
    """Pipeline complet de pr√©processing"""
    # 1. Nettoyage ‚Üí 2. Tokenisation ‚Üí 3. Filtrage ‚Üí 4. Lemmatisation
    
def extract_sentences(self, text: str) -> List[str]:
    """Extraction de phrases avec fallback"""
    # NLTK sent_tokenize ou regex fallback
    
def get_word_frequency(self, texts: List[str], top_n: int = 20) -> dict:
    """Calcul de fr√©quence des mots"""
```

#### **Gestion des Fallbacks**
```python
# Imports avec gestion d'erreurs
try:
    import nltk
    NLTK_AVAILABLE = True
except ImportError:
    NLTK_AVAILABLE = False

# Utilisation conditionnelle
if NLTK_AVAILABLE:
    tokens = word_tokenize(text)
else:
    tokens = re.findall(r'\b\w+\b', text)  # Fallback regex
```

### **2. sentiment_analysis.py**

#### **Classe SentimentAnalyzer**
```python
class SentimentAnalyzer:
    def __init__(self):
        self.models = {}
        self.vectorizer = None
        self.label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}
        
        # Configuration inline (pas de fichier config externe)
        self.config = {
            "model_name": "cardiffnlp/twitter-roberta-base-sentiment-latest",
            "max_length": 512
        }
```

#### **M√©thodes d'Analyse**
```python
def textblob_sentiment(self, text: str) -> Dict[str, float]:
    """Analyse avec TextBlob - rapide et simple"""
    
def transformer_sentiment(self, text: str) -> Dict[str, Union[str, float]]:
    """Analyse avec mod√®les Transformer (si disponible)"""
    
def predict_sentiment(self, text: Union[str, List[str]], method: str = 'textblob'):
    """Interface unifi√©e pour toutes les m√©thodes"""
```

---

## ü§ñ **Algorithmes Impl√©ment√©s**

### **1. TF-IDF (Term Frequency-Inverse Document Frequency)**

#### **Impl√©mentation**
```python
from sklearn.feature_extraction.text import TfidfVectorizer

def create_tfidf_vectorizer():
    return TfidfVectorizer(
        max_features=1000,           # Vocabulaire limit√©
        ngram_range=(1, 2),          # Uni et bigrammes
        stop_words='english',        # Suppression mots vides
        lowercase=True,              # Normalisation casse
        min_df=2,                    # Fr√©quence minimum
        max_df=0.95                  # Fr√©quence maximum
    )
```

#### **Utilisation**
```python
# Entra√Ænement
vectorizer = create_tfidf_vectorizer()
tfidf_matrix = vectorizer.fit_transform(documents)

# Transformation nouveau texte
new_vector = vectorizer.transform([new_text])
```

### **2. LDA (Latent Dirichlet Allocation)**

#### **Configuration**
```python
from sklearn.decomposition import LatentDirichletAllocation

def create_lda_model(n_topics=5):
    return LatentDirichletAllocation(
        n_components=n_topics,       # Nombre de sujets
        random_state=42,             # Reproductibilit√©
        max_iter=100,               # It√©rations maximum
        learning_method='batch',     # M√©thode d'apprentissage
        learning_offset=50.0,        # Param√®tre d'apprentissage
        doc_topic_prior=None,        # Prior Dirichlet documents
        topic_word_prior=None        # Prior Dirichlet mots
    )
```

#### **Pipeline Complet**
```python
def topic_modeling_pipeline(texts, n_topics=3):
    # 1. Pr√©processing
    preprocessor = TextPreprocessor()
    processed_texts = [preprocessor.preprocess_text(text) for text in texts]
    
    # 2. Vectorisation TF-IDF
    vectorizer = create_tfidf_vectorizer()
    tfidf_matrix = vectorizer.fit_transform(processed_texts)
    
    # 3. Mod√©lisation LDA
    lda_model = create_lda_model(n_topics)
    lda_model.fit(tfidf_matrix)
    
    # 4. Extraction des sujets
    feature_names = vectorizer.get_feature_names_out()
    topics = extract_topics(lda_model, feature_names)
    
    return lda_model, vectorizer, topics
```

### **3. Named Entity Recognition**

#### **M√©thode spaCy (si disponible)**
```python
def spacy_ner(text):
    if SPACY_AVAILABLE and nlp:
        doc = nlp(text)
        entities = [(ent.text, ent.label_) for ent in doc.ents]
        return entities
    return []
```

#### **M√©thode Pattern-based (fallback)**
```python
def pattern_based_ner(text):
    entities = []
    
    # Patterns pour diff√©rents types d'entit√©s
    patterns = {
        'PERSON': r'\b[A-Z][a-z]+ [A-Z][a-z]+\b',
        'ORG': r'\b[A-Z][a-z]+ (?:Inc|Corp|Ltd|LLC)\b',
        'GPE': r'\b(?:Paris|London|New York|Tokyo)\b',
        'DATE': r'\b\d{4}\b|\b\d{1,2}/\d{1,2}/\d{4}\b'
    }
    
    for entity_type, pattern in patterns.items():
        matches = re.finditer(pattern, text)
        for match in matches:
            entities.append((match.group(), entity_type))
    
    return entities
```

### **4. Text Summarization**

#### **M√©thode TF-IDF**
```python
def tfidf_based_summary(text, num_sentences=3):
    # 1. Segmentation en phrases
    sentences = sent_tokenize(text)
    
    # 2. Vectorisation TF-IDF
    vectorizer = TfidfVectorizer(stop_words='english')
    tfidf_matrix = vectorizer.fit_transform(sentences)
    
    # 3. Calcul des scores de phrases
    sentence_scores = tfidf_matrix.sum(axis=1).A1
    
    # 4. S√©lection des meilleures phrases
    ranked_sentences = sorted(enumerate(sentence_scores), 
                            key=lambda x: x[1], reverse=True)
    
    # 5. Reconstruction du r√©sum√©
    summary_indices = sorted([i for i, _ in ranked_sentences[:num_sentences]])
    summary = ' '.join([sentences[i] for i in summary_indices])
    
    return summary
```

#### **M√©thode Frequency-based**
```python
def frequency_based_summary(text, num_sentences=3):
    # 1. Pr√©processing et comptage des mots
    words = preprocess_text(text).split()
    word_freq = Counter(words)
    
    # 2. Score des phrases bas√© sur la fr√©quence
    sentences = sent_tokenize(text)
    sentence_scores = {}
    
    for sentence in sentences:
        sentence_words = preprocess_text(sentence).split()
        score = sum(word_freq[word] for word in sentence_words)
        sentence_scores[sentence] = score / len(sentence_words)
    
    # 3. S√©lection des meilleures phrases
    ranked_sentences = sorted(sentence_scores.items(), 
                            key=lambda x: x[1], reverse=True)
    
    return ' '.join([sent for sent, _ in ranked_sentences[:num_sentences]])
```

---

## üõ°Ô∏è **Gestion des Erreurs**

### **Strat√©gie de Robustesse**
```python
def safe_execution(func, fallback_func=None, *args, **kwargs):
    """Ex√©cution s√©curis√©e avec fallback"""
    try:
        return func(*args, **kwargs)
    except Exception as e:
        st.warning(f"Primary method failed: {e}")
        if fallback_func:
            try:
                return fallback_func(*args, **kwargs)
            except Exception as e2:
                st.error(f"Fallback method also failed: {e2}")
        return None
```

### **Validation des Entr√©es**
```python
def validate_text_input(text):
    """Validation et nettoyage des entr√©es utilisateur"""
    if not text or not text.strip():
        raise ValueError("Text cannot be empty")
    
    if len(text) < 10:
        raise ValueError("Text too short for meaningful analysis")
    
    if len(text) > 50000:
        st.warning("Text is very long, truncating to 50,000 characters")
        text = text[:50000]
    
    return text.strip()
```

### **Gestion des Mod√®les Manquants**
```python
def load_model_safely(model_path):
    """Chargement s√©curis√© des mod√®les"""
    try:
        with open(model_path, 'rb') as f:
            model = pickle.load(f)
        return model
    except FileNotFoundError:
        st.warning(f"Model not found: {model_path}")
        return None
    except Exception as e:
        st.error(f"Error loading model: {e}")
        return None
```

---

## ‚ö° **Performance et Optimisation**

### **Caching Streamlit**
```python
@st.cache_data
def load_and_preprocess_text(text):
    """Cache le pr√©processing pour √©viter les recalculs"""
    preprocessor = TextPreprocessor()
    return preprocessor.preprocess_text(text)

@st.cache_resource
def load_models():
    """Cache le chargement des mod√®les (une seule fois)"""
    models = {}
    models['lda'] = load_model_safely('models/lda_model.pkl')
    models['tfidf'] = load_model_safely('models/tfidf_vectorizer.pkl')
    return models
```

### **Optimisation M√©moire**
```python
def process_large_text(text, chunk_size=1000):
    """Traitement par chunks pour les gros textes"""
    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]
    results = []
    
    for chunk in chunks:
        result = process_chunk(chunk)
        results.append(result)
    
    return combine_results(results)
```

### **Lazy Loading**
```python
class LazyNLPModules:
    """Chargement paresseux des modules lourds"""
    def __init__(self):
        self._spacy_model = None
        self._transformer_model = None
    
    @property
    def spacy_model(self):
        if self._spacy_model is None:
            self._spacy_model = self._load_spacy()
        return self._spacy_model
```

---

## üîß **Extension et Personnalisation**

### **Ajouter une Nouvelle Fonctionnalit√©**

#### **1. Cr√©er le Module**
```python
# src/new_feature.py
class NewFeatureAnalyzer:
    def __init__(self):
        pass
    
    def analyze(self, text):
        # Impl√©mentation de la nouvelle fonctionnalit√©
        return results
```

#### **2. Int√©grer dans app.py**
```python
# Dans load_modules()
try:
    from new_feature import NewFeatureAnalyzer
    modules['new_feature'] = NewFeatureAnalyzer()
    st.sidebar.success("‚úÖ New Feature loaded")
except Exception as e:
    st.sidebar.error(f"‚ùå New Feature failed: {e}")
    modules['new_feature'] = None

# Ajouter dans la navigation
analysis_type = st.sidebar.selectbox(
    "Choose Analysis Tool:",
    ["üè† Home", "üìä Data Exploration", ..., "üÜï New Feature"]
)

# Cr√©er la fonction d'affichage
def show_new_feature(text, modules):
    st.markdown('<h2 class="feature-header">üÜï New Feature</h2>', 
                unsafe_allow_html=True)
    
    if modules['new_feature']:
        results = modules['new_feature'].analyze(text)
        # Affichage des r√©sultats
    else:
        st.error("New Feature not available")
```

### **Personnaliser les Styles**
```python
def load_custom_css():
    st.markdown("""
    <style>
    .feature-header {
        color: #1f77b4;
        font-size: 2rem;
        margin-bottom: 1rem;
        border-bottom: 2px solid #1f77b4;
    }
    
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
    }
    </style>
    """, unsafe_allow_html=True)
```

### **Ajouter de Nouveaux Mod√®les**
```python
# Entra√Æner et sauvegarder un nouveau mod√®le
def train_and_save_model(data, model_path):
    model = SomeMLModel()
    model.fit(data)
    
    with open(model_path, 'wb') as f:
        pickle.dump(model, f)
    
    print(f"Model saved to {model_path}")

# Charger dans l'application
@st.cache_resource
def load_custom_model():
    return load_model_safely('models/custom_model.pkl')
```

---

## üìä **Monitoring et Debugging**

### **Logging**
```python
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def analyze_with_logging(text, method):
    logger.info(f"Starting analysis with method: {method}")
    start_time = time.time()
    
    try:
        result = perform_analysis(text, method)
        duration = time.time() - start_time
        logger.info(f"Analysis completed in {duration:.2f}s")
        return result
    except Exception as e:
        logger.error(f"Analysis failed: {e}")
        raise
```

### **M√©triques de Performance**
```python
def track_performance(func):
    """D√©corateur pour tracker les performances"""
    def wrapper(*args, **kwargs):
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss
        
        result = func(*args, **kwargs)
        
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss
        
        st.sidebar.metric("Execution Time", f"{end_time - start_time:.2f}s")
        st.sidebar.metric("Memory Usage", f"{(end_memory - start_memory) / 1024 / 1024:.1f} MB")
        
        return result
    return wrapper
```

**Votre NLP Agent est maintenant techniquement document√© et pr√™t pour l'extension! üöÄ**
