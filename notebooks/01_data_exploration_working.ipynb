{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Exploration et Pr√©processing des Donn√©es - Version Fonctionnelle\n",
    "\n",
    "Ce notebook fournit une analyse compl√®te du dataset d'actualit√©s avec toutes les fonctionnalit√©s NLP.\n",
    "\n",
    "## üéØ Objectifs:\n",
    "- ‚úÖ Charger et explorer le dataset\n",
    "- üìà Analyser les distributions et statistiques\n",
    "- üî§ Pr√©processer le texte\n",
    "- üìù Analyser la fr√©quence des mots\n",
    "- ‚òÅÔ∏è Cr√©er des visualisations avanc√©es\n",
    "- üíæ Sauvegarder les donn√©es trait√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Import des biblioth√®ques essentielles\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# üé® Configuration des graphiques\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print('‚úÖ Biblioth√®ques import√©es avec succ√®s!')\n",
    "print('üöÄ Pr√™t pour l\\'analyse NLP!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ 1. Chargement et Exploration du Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Charger le dataset\n",
    "try:\n",
    "    df = pd.read_csv('../data/sample/sample_news_data.csv')\n",
    "    print(f\"‚úÖ Dataset charg√© avec succ√®s!\")\n",
    "    print(f\"üìè Forme du dataset: {df.shape}\")\n",
    "    print(f\"üìã Colonnes: {df.columns.tolist()}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Fichier non trouv√©. Cr√©ation d'un dataset d'exemple...\")\n",
    "    # Cr√©er un dataset d'exemple si le fichier n'existe pas\n",
    "    df = pd.DataFrame({\n",
    "        'id': range(1, 11),\n",
    "        'title': [f'Article {i}' for i in range(1, 11)],\n",
    "        'text': [f'Ceci est le contenu de l\\'article {i}. Il contient du texte int√©ressant pour l\\'analyse NLP.' for i in range(1, 11)],\n",
    "        'category': ['technology', 'sports', 'politics'] * 3 + ['health'],\n",
    "        'sentiment': ['positive', 'negative', 'neutral'] * 3 + ['positive']\n",
    "    })\n",
    "    print(f\"‚úÖ Dataset d'exemple cr√©√©: {df.shape}\")\n",
    "\n",
    "# üëÄ Aper√ßu des donn√©es\n",
    "print(\"\\nüìã Premi√®res lignes:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Informations d√©taill√©es sur le dataset\n",
    "print(\"üìä INFORMATIONS SUR LE DATASET\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìè Nombre de lignes: {len(df)}\")\n",
    "print(f\"üìã Nombre de colonnes: {len(df.columns)}\")\n",
    "print(f\"üíæ Taille m√©moire: {df.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "\n",
    "print(\"\\nüîç Types de donn√©es:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n‚ùì Valeurs manquantes:\")\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() == 0:\n",
    "    print(\"‚úÖ Aucune valeur manquante!\")\n",
    "else:\n",
    "    print(missing[missing > 0])\n",
    "\n",
    "print(\"\\nüìà Statistiques descriptives:\")\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù 2. Analyse du Texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìè Calcul des m√©triques de texte\n",
    "print(\"üìä CALCUL DES M√âTRIQUES DE TEXTE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Longueur en caract√®res\n",
    "df['text_length'] = df['text'].str.len()\n",
    "\n",
    "# Nombre de mots\n",
    "df['word_count'] = df['text'].str.split().str.len()\n",
    "\n",
    "# Nombre de phrases (approximatif)\n",
    "df['sentence_count'] = df['text'].str.count(r'[.!?]') + 1\n",
    "\n",
    "# Nombre de mots uniques\n",
    "df['unique_words'] = df['text'].apply(lambda x: len(set(x.lower().split())))\n",
    "\n",
    "# Richesse lexicale (ratio mots uniques / total mots)\n",
    "df['lexical_diversity'] = df['unique_words'] / df['word_count']\n",
    "\n",
    "# Statistiques\n",
    "metrics = ['text_length', 'word_count', 'sentence_count', 'unique_words', 'lexical_diversity']\n",
    "stats = df[metrics].describe()\n",
    "\n",
    "print(f\"üìè Longueur moyenne: {stats.loc['mean', 'text_length']:.1f} caract√®res\")\n",
    "print(f\"üìù Mots moyens: {stats.loc['mean', 'word_count']:.1f} mots\")\n",
    "print(f\"üìÑ Phrases moyennes: {stats.loc['mean', 'sentence_count']:.1f} phrases\")\n",
    "print(f\"üî§ Mots uniques moyens: {stats.loc['mean', 'unique_words']:.1f}\")\n",
    "print(f\"üéØ Diversit√© lexicale moyenne: {stats.loc['mean', 'lexical_diversity']:.3f}\")\n",
    "\n",
    "print(\"\\nüìä Statistiques compl√®tes:\")\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 3. Distributions et Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìà Visualisation des distributions de texte\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('üìä Distributions des M√©triques de Texte', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Distribution de la longueur\n",
    "axes[0,0].hist(df['text_length'], bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0,0].axvline(df['text_length'].mean(), color='red', linestyle='--', \n",
    "                  label=f'Moyenne: {df[\"text_length\"].mean():.1f}')\n",
    "axes[0,0].set_title('üìè Distribution de la Longueur (caract√®res)')\n",
    "axes[0,0].set_xlabel('Nombre de caract√®res')\n",
    "axes[0,0].set_ylabel('Fr√©quence')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution du nombre de mots\n",
    "axes[0,1].hist(df['word_count'], bins=15, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[0,1].axvline(df['word_count'].mean(), color='red', linestyle='--',\n",
    "                  label=f'Moyenne: {df[\"word_count\"].mean():.1f}')\n",
    "axes[0,1].set_title('üìù Distribution du Nombre de Mots')\n",
    "axes[0,1].set_xlabel('Nombre de mots')\n",
    "axes[0,1].set_ylabel('Fr√©quence')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution des phrases\n",
    "axes[1,0].hist(df['sentence_count'], bins=10, alpha=0.7, color='orange', edgecolor='black')\n",
    "axes[1,0].axvline(df['sentence_count'].mean(), color='red', linestyle='--',\n",
    "                  label=f'Moyenne: {df[\"sentence_count\"].mean():.1f}')\n",
    "axes[1,0].set_title('üìÑ Distribution du Nombre de Phrases')\n",
    "axes[1,0].set_xlabel('Nombre de phrases')\n",
    "axes[1,0].set_ylabel('Fr√©quence')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Diversit√© lexicale\n",
    "axes[1,1].hist(df['lexical_diversity'], bins=10, alpha=0.7, color='purple', edgecolor='black')\n",
    "axes[1,1].axvline(df['lexical_diversity'].mean(), color='red', linestyle='--',\n",
    "                  label=f'Moyenne: {df[\"lexical_diversity\"].mean():.3f}')\n",
    "axes[1,1].set_title('üéØ Distribution de la Diversit√© Lexicale')\n",
    "axes[1,1].set_xlabel('Ratio mots uniques / total')\n",
    "axes[1,1].set_ylabel('Fr√©quence')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üè∑Ô∏è Analyse des cat√©gories et sentiments\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Distribution des cat√©gories\n",
    "category_counts = df['category'].value_counts()\n",
    "colors1 = plt.cm.Set3(np.linspace(0, 1, len(category_counts)))\n",
    "\n",
    "bars1 = ax1.bar(category_counts.index, category_counts.values, color=colors1, alpha=0.8, edgecolor='black')\n",
    "ax1.set_title('üè∑Ô∏è Distribution des Cat√©gories', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Cat√©gorie')\n",
    "ax1.set_ylabel('Nombre d\\'articles')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "             f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Distribution des sentiments\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "colors2 = {'positive': 'lightgreen', 'negative': 'lightcoral', 'neutral': 'lightblue'}\n",
    "sentiment_colors = [colors2.get(sent, 'gray') for sent in sentiment_counts.index]\n",
    "\n",
    "bars2 = ax2.bar(sentiment_counts.index, sentiment_counts.values, color=sentiment_colors, alpha=0.8, edgecolor='black')\n",
    "ax2.set_title('üòä Distribution des Sentiments', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Sentiment')\n",
    "ax2.set_ylabel('Nombre d\\'articles')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "             f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Afficher les statistiques\n",
    "print(\"üìä DISTRIBUTION DES CAT√âGORIES:\")\n",
    "for cat, count in category_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"  üè∑Ô∏è {cat}: {count} articles ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\nüòä DISTRIBUTION DES SENTIMENTS:\")\n",
    "for sent, count in sentiment_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    emoji = {'positive': 'üòä', 'negative': 'üòû', 'neutral': 'üòê'}.get(sent, '‚ùì')\n",
    "    print(f\"  {emoji} {sent}: {count} articles ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî§ 4. Pr√©processing du Texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è Fonction de pr√©processing avanc√©e\n",
    "def advanced_preprocess(text):\n",
    "    \"\"\"Pr√©processing avanc√© du texte\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convertir en minuscules\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Supprimer les URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Supprimer les mentions et hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # Supprimer les chiffres\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Supprimer la ponctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Supprimer les espaces multiples\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Appliquer le pr√©processing\n",
    "print(\"üîÑ Application du pr√©processing...\")\n",
    "df['processed_text'] = df['text'].apply(advanced_preprocess)\n",
    "\n",
    "# Calculer les nouvelles m√©triques\n",
    "df['processed_length'] = df['processed_text'].str.len()\n",
    "df['processed_words'] = df['processed_text'].str.split().str.len()\n",
    "\n",
    "print(\"‚úÖ Pr√©processing termin√©!\")\n",
    "print(f\"üìè R√©duction moyenne de longueur: {((df['text_length'] - df['processed_length']) / df['text_length'] * 100).mean():.1f}%\")\n",
    "\n",
    "# Exemples avant/apr√®s\n",
    "print(\"\\nüìù EXEMPLES DE TRANSFORMATION:\")\n",
    "print(\"=\" * 50)\n",
    "for i in range(min(3, len(df))):\n",
    "    print(f\"\\nüî∏ Exemple {i+1}:\")\n",
    "    print(f\"   üìÑ Original: {df.iloc[i]['text'][:100]}...\")\n",
    "    print(f\"   üîß Trait√©: {df.iloc[i]['processed_text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 5. Analyse de Fr√©quence des Mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Analyse de fr√©quence des mots\n",
    "print(\"üìä ANALYSE DE FR√âQUENCE DES MOTS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Mots vides fran√ßais et anglais courants\n",
    "stop_words = {\n",
    "    'le', 'de', 'et', '√†', 'un', 'il', '√™tre', 'et', 'en', 'avoir', 'que', 'pour',\n",
    "    'dans', 'ce', 'son', 'une', 'sur', 'avec', 'ne', 'se', 'pas', 'tout', 'plus',\n",
    "    'par', 'grand', 'en', '√™tre', 'et', 'en', 'avoir', 'que', 'pour', 'du', 'des',\n",
    "    'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',\n",
    "    'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n",
    "    'this', 'that', 'these', 'those', 'a', 'an', 'as', 'if', 'it', 'its'\n",
    "}\n",
    "\n",
    "# Extraire tous les mots\n",
    "all_words = []\n",
    "for text in df['processed_text']:\n",
    "    if isinstance(text, str):\n",
    "        words = text.split()\n",
    "        # Filtrer les mots courts et les mots vides\n",
    "        words = [word for word in words if len(word) > 2 and word not in stop_words]\n",
    "        all_words.extend(words)\n",
    "\n",
    "# Compter les fr√©quences\n",
    "word_freq = Counter(all_words)\n",
    "top_words = word_freq.most_common(20)\n",
    "\n",
    "print(f\"üìù Total de mots uniques: {len(word_freq)}\")\n",
    "print(f\"üìä Total d'occurrences: {sum(word_freq.values())}\")\n",
    "\n",
    "print(\"\\nüèÜ TOP 20 DES MOTS LES PLUS FR√âQUENTS:\")\n",
    "for i, (word, freq) in enumerate(top_words, 1):\n",
    "    percentage = (freq / sum(word_freq.values())) * 100\n",
    "    print(f\"  {i:2d}. {word:<15} : {freq:3d} fois ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualisation de la fr√©quence des mots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Graphique en barres horizontales\n",
    "words, frequencies = zip(*top_words)\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(words)))\n",
    "\n",
    "bars = ax1.barh(range(len(words)), frequencies, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax1.set_yticks(range(len(words)))\n",
    "ax1.set_yticklabels(words)\n",
    "ax1.set_xlabel('Fr√©quence')\n",
    "ax1.set_title('üèÜ Top 20 des Mots les Plus Fr√©quents', fontsize=14, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for i, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    ax1.text(width + 0.1, bar.get_y() + bar.get_height()/2,\n",
    "             f'{int(width)}', ha='left', va='center', fontweight='bold')\n",
    "\n",
    "# Distribution de Zipf (loi de puissance)\n",
    "ranks = np.arange(1, len(top_words) + 1)\n",
    "ax2.loglog(ranks, frequencies, 'bo-', alpha=0.7, linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Rang (log)')\n",
    "ax2.set_ylabel('Fr√©quence (log)')\n",
    "ax2.set_title('üìà Distribution de Zipf', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Ligne de tendance\n",
    "z = np.polyfit(np.log(ranks), np.log(frequencies), 1)\n",
    "p = np.poly1d(z)\n",
    "ax2.plot(ranks, np.exp(p(np.log(ranks))), \"r--\", alpha=0.8, linewidth=2, \n",
    "         label=f'Pente: {z[0]:.2f}')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Coefficient de Zipf: {abs(z[0]):.2f} (id√©al ‚âà 1.0)\")\n",
    "if abs(z[0]) > 0.8:\n",
    "    print(\"‚úÖ Distribution proche de la loi de Zipf (texte naturel)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Distribution √©loign√©e de la loi de Zipf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚òÅÔ∏è 6. Nuage de Mots et Visualisations Avanc√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚òÅÔ∏è Cr√©ation de nuages de mots\n",
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "    \n",
    "    # Combiner tout le texte pr√©process√©\n",
    "    combined_text = ' '.join(df['processed_text'].dropna())\n",
    "    \n",
    "    # Cr√©er le nuage de mots principal\n",
    "    wordcloud = WordCloud(\n",
    "        width=1200, \n",
    "        height=600, \n",
    "        background_color='white',\n",
    "        max_words=100,\n",
    "        colormap='viridis',\n",
    "        relative_scaling=0.5,\n",
    "        min_font_size=10\n",
    "    ).generate(combined_text)\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('‚òÅÔ∏è Nuage de Mots Global', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Nuages de mots par sentiment\n",
    "    sentiments = df['sentiment'].unique()\n",
    "    if len(sentiments) > 1:\n",
    "        fig, axes = plt.subplots(1, len(sentiments), figsize=(5*len(sentiments), 6))\n",
    "        if len(sentiments) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        colors = {'positive': 'Greens', 'negative': 'Reds', 'neutral': 'Blues'}\n",
    "        \n",
    "        for i, sentiment in enumerate(sentiments):\n",
    "            sentiment_text = ' '.join(df[df['sentiment'] == sentiment]['processed_text'].dropna())\n",
    "            \n",
    "            if sentiment_text.strip():  # V√©rifier que le texte n'est pas vide\n",
    "                wc = WordCloud(\n",
    "                    width=400, height=300,\n",
    "                    background_color='white',\n",
    "                    max_words=50,\n",
    "                    colormap=colors.get(sentiment, 'viridis')\n",
    "                ).generate(sentiment_text)\n",
    "                \n",
    "                axes[i].imshow(wc, interpolation='bilinear')\n",
    "                axes[i].axis('off')\n",
    "                emoji = {'positive': 'üòä', 'negative': 'üòû', 'neutral': 'üòê'}.get(sentiment, '‚ùì')\n",
    "                axes[i].set_title(f'{emoji} {sentiment.title()}', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        plt.suptitle('‚òÅÔ∏è Nuages de Mots par Sentiment', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Nuages de mots cr√©√©s avec succ√®s!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è WordCloud non disponible. Affichage textuel des mots fr√©quents:\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    for word, freq in top_words[:15]:\n",
    "        bar_length = int((freq / max(frequencies)) * 40)\n",
    "        bar = \"‚ñà\" * bar_length\n",
    "        print(f\"{word:<15} {bar} {freq}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ 7. Sauvegarde et R√©sum√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíæ Sauvegarde des donn√©es pr√©process√©es\n",
    "import os\n",
    "\n",
    "# Cr√©er le dossier processed s'il n'existe pas\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Sauvegarder le dataset enrichi\n",
    "output_file = '../data/processed/processed_news_data.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"üíæ Donn√©es sauvegard√©es dans: {output_file}\")\n",
    "print(f\"üìä Taille du fichier: {os.path.getsize(output_file) / 1024:.2f} KB\")\n",
    "\n",
    "# Sauvegarder les statistiques de mots\n",
    "word_stats = pd.DataFrame(top_words, columns=['word', 'frequency'])\n",
    "word_stats['percentage'] = (word_stats['frequency'] / sum(word_freq.values())) * 100\n",
    "word_stats.to_csv('../data/processed/word_frequencies.csv', index=False)\n",
    "\n",
    "print(\"üìù Fr√©quences des mots sauvegard√©es dans: ../data/processed/word_frequencies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìã R√©sum√© final de l'analyse\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ R√âSUM√â DE L'ANALYSE EXPLORATOIRE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìä DATASET:\")\n",
    "print(f\"   üìè {len(df)} articles analys√©s\")\n",
    "print(f\"   üè∑Ô∏è {df['category'].nunique()} cat√©gories: {', '.join(df['category'].unique())}\")\n",
    "print(f\"   üòä {df['sentiment'].nunique()} sentiments: {', '.join(df['sentiment'].unique())}\")\n",
    "\n",
    "print(f\"\\nüìù M√âTRIQUES DE TEXTE:\")\n",
    "print(f\"   üìè Longueur moyenne: {df['text_length'].mean():.1f} caract√®res\")\n",
    "print(f\"   üî§ Mots moyens: {df['word_count'].mean():.1f} mots\")\n",
    "print(f\"   üìÑ Phrases moyennes: {df['sentence_count'].mean():.1f} phrases\")\n",
    "print(f\"   üéØ Diversit√© lexicale: {df['lexical_diversity'].mean():.3f}\")\n",
    "\n",
    "print(f\"\\nüîç ANALYSE LEXICALE:\")\n",
    "print(f\"   üìö {len(word_freq)} mots uniques identifi√©s\")\n",
    "print(f\"   üèÜ Mot le plus fr√©quent: '{top_words[0][0]}' ({top_words[0][1]} fois)\")\n",
    "print(f\"   üìä Coefficient de Zipf: {abs(z[0]):.2f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ FICHIERS G√âN√âR√âS:\")\n",
    "print(f\"   üíæ Dataset pr√©process√©: processed_news_data.csv\")\n",
    "print(f\"   üìä Fr√©quences des mots: word_frequencies.csv\")\n",
    "print(f\"   üìà Visualisations: Graphiques interactifs g√©n√©r√©s\")\n",
    "\n",
    "print(f\"\\nüöÄ PROCHAINES √âTAPES RECOMMAND√âES:\")\n",
    "print(f\"   1Ô∏è‚É£ Analyse de sentiment avec TextBlob/VADER\")\n",
    "print(f\"   2Ô∏è‚É£ Mod√©lisation de sujets avec LDA/BERTopic\")\n",
    "print(f\"   3Ô∏è‚É£ Reconnaissance d'entit√©s nomm√©es (NER)\")\n",
    "print(f\"   4Ô∏è‚É£ R√©sum√© automatique de texte\")\n",
    "print(f\"   5Ô∏è‚É£ Classification automatique\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ ANALYSE TERMIN√âE AVEC SUCC√àS!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
