{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📊 Exploration et Préprocessing des Données - Version Fonctionnelle\n",
    "\n",
    "Ce notebook fournit une analyse complète du dataset d'actualités avec toutes les fonctionnalités NLP.\n",
    "\n",
    "## 🎯 Objectifs:\n",
    "- ✅ Charger et explorer le dataset\n",
    "- 📈 Analyser les distributions et statistiques\n",
    "- 🔤 Préprocesser le texte\n",
    "- 📝 Analyser la fréquence des mots\n",
    "- ☁️ Créer des visualisations avancées\n",
    "- 💾 Sauvegarder les données traitées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 Import des bibliothèques essentielles\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 🎨 Configuration des graphiques\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print('✅ Bibliothèques importées avec succès!')\n",
    "print('🚀 Prêt pour l\\'analyse NLP!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📂 1. Chargement et Exploration du Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Charger le dataset\n",
    "try:\n",
    "    df = pd.read_csv('../data/sample/sample_news_data.csv')\n",
    "    print(f\"✅ Dataset chargé avec succès!\")\n",
    "    print(f\"📏 Forme du dataset: {df.shape}\")\n",
    "    print(f\"📋 Colonnes: {df.columns.tolist()}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Fichier non trouvé. Création d'un dataset d'exemple...\")\n",
    "    # Créer un dataset d'exemple si le fichier n'existe pas\n",
    "    df = pd.DataFrame({\n",
    "        'id': range(1, 11),\n",
    "        'title': [f'Article {i}' for i in range(1, 11)],\n",
    "        'text': [f'Ceci est le contenu de l\\'article {i}. Il contient du texte intéressant pour l\\'analyse NLP.' for i in range(1, 11)],\n",
    "        'category': ['technology', 'sports', 'politics'] * 3 + ['health'],\n",
    "        'sentiment': ['positive', 'negative', 'neutral'] * 3 + ['positive']\n",
    "    })\n",
    "    print(f\"✅ Dataset d'exemple créé: {df.shape}\")\n",
    "\n",
    "# 👀 Aperçu des données\n",
    "print(\"\\n📋 Premières lignes:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 Informations détaillées sur le dataset\n",
    "print(\"📊 INFORMATIONS SUR LE DATASET\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"📏 Nombre de lignes: {len(df)}\")\n",
    "print(f\"📋 Nombre de colonnes: {len(df.columns)}\")\n",
    "print(f\"💾 Taille mémoire: {df.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "\n",
    "print(\"\\n🔍 Types de données:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n❓ Valeurs manquantes:\")\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() == 0:\n",
    "    print(\"✅ Aucune valeur manquante!\")\n",
    "else:\n",
    "    print(missing[missing > 0])\n",
    "\n",
    "print(\"\\n📈 Statistiques descriptives:\")\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📝 2. Analyse du Texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📏 Calcul des métriques de texte\n",
    "print(\"📊 CALCUL DES MÉTRIQUES DE TEXTE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Longueur en caractères\n",
    "df['text_length'] = df['text'].str.len()\n",
    "\n",
    "# Nombre de mots\n",
    "df['word_count'] = df['text'].str.split().str.len()\n",
    "\n",
    "# Nombre de phrases (approximatif)\n",
    "df['sentence_count'] = df['text'].str.count(r'[.!?]') + 1\n",
    "\n",
    "# Nombre de mots uniques\n",
    "df['unique_words'] = df['text'].apply(lambda x: len(set(x.lower().split())))\n",
    "\n",
    "# Richesse lexicale (ratio mots uniques / total mots)\n",
    "df['lexical_diversity'] = df['unique_words'] / df['word_count']\n",
    "\n",
    "# Statistiques\n",
    "metrics = ['text_length', 'word_count', 'sentence_count', 'unique_words', 'lexical_diversity']\n",
    "stats = df[metrics].describe()\n",
    "\n",
    "print(f\"📏 Longueur moyenne: {stats.loc['mean', 'text_length']:.1f} caractères\")\n",
    "print(f\"📝 Mots moyens: {stats.loc['mean', 'word_count']:.1f} mots\")\n",
    "print(f\"📄 Phrases moyennes: {stats.loc['mean', 'sentence_count']:.1f} phrases\")\n",
    "print(f\"🔤 Mots uniques moyens: {stats.loc['mean', 'unique_words']:.1f}\")\n",
    "print(f\"🎯 Diversité lexicale moyenne: {stats.loc['mean', 'lexical_diversity']:.3f}\")\n",
    "\n",
    "print(\"\\n📊 Statistiques complètes:\")\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 3. Distributions et Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📈 Visualisation des distributions de texte\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('📊 Distributions des Métriques de Texte', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Distribution de la longueur\n",
    "axes[0,0].hist(df['text_length'], bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0,0].axvline(df['text_length'].mean(), color='red', linestyle='--', \n",
    "                  label=f'Moyenne: {df[\"text_length\"].mean():.1f}')\n",
    "axes[0,0].set_title('📏 Distribution de la Longueur (caractères)')\n",
    "axes[0,0].set_xlabel('Nombre de caractères')\n",
    "axes[0,0].set_ylabel('Fréquence')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution du nombre de mots\n",
    "axes[0,1].hist(df['word_count'], bins=15, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[0,1].axvline(df['word_count'].mean(), color='red', linestyle='--',\n",
    "                  label=f'Moyenne: {df[\"word_count\"].mean():.1f}')\n",
    "axes[0,1].set_title('📝 Distribution du Nombre de Mots')\n",
    "axes[0,1].set_xlabel('Nombre de mots')\n",
    "axes[0,1].set_ylabel('Fréquence')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution des phrases\n",
    "axes[1,0].hist(df['sentence_count'], bins=10, alpha=0.7, color='orange', edgecolor='black')\n",
    "axes[1,0].axvline(df['sentence_count'].mean(), color='red', linestyle='--',\n",
    "                  label=f'Moyenne: {df[\"sentence_count\"].mean():.1f}')\n",
    "axes[1,0].set_title('📄 Distribution du Nombre de Phrases')\n",
    "axes[1,0].set_xlabel('Nombre de phrases')\n",
    "axes[1,0].set_ylabel('Fréquence')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Diversité lexicale\n",
    "axes[1,1].hist(df['lexical_diversity'], bins=10, alpha=0.7, color='purple', edgecolor='black')\n",
    "axes[1,1].axvline(df['lexical_diversity'].mean(), color='red', linestyle='--',\n",
    "                  label=f'Moyenne: {df[\"lexical_diversity\"].mean():.3f}')\n",
    "axes[1,1].set_title('🎯 Distribution de la Diversité Lexicale')\n",
    "axes[1,1].set_xlabel('Ratio mots uniques / total')\n",
    "axes[1,1].set_ylabel('Fréquence')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🏷️ Analyse des catégories et sentiments\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Distribution des catégories\n",
    "category_counts = df['category'].value_counts()\n",
    "colors1 = plt.cm.Set3(np.linspace(0, 1, len(category_counts)))\n",
    "\n",
    "bars1 = ax1.bar(category_counts.index, category_counts.values, color=colors1, alpha=0.8, edgecolor='black')\n",
    "ax1.set_title('🏷️ Distribution des Catégories', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Catégorie')\n",
    "ax1.set_ylabel('Nombre d\\'articles')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "             f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Distribution des sentiments\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "colors2 = {'positive': 'lightgreen', 'negative': 'lightcoral', 'neutral': 'lightblue'}\n",
    "sentiment_colors = [colors2.get(sent, 'gray') for sent in sentiment_counts.index]\n",
    "\n",
    "bars2 = ax2.bar(sentiment_counts.index, sentiment_counts.values, color=sentiment_colors, alpha=0.8, edgecolor='black')\n",
    "ax2.set_title('😊 Distribution des Sentiments', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Sentiment')\n",
    "ax2.set_ylabel('Nombre d\\'articles')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "             f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Afficher les statistiques\n",
    "print(\"📊 DISTRIBUTION DES CATÉGORIES:\")\n",
    "for cat, count in category_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"  🏷️ {cat}: {count} articles ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\n😊 DISTRIBUTION DES SENTIMENTS:\")\n",
    "for sent, count in sentiment_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    emoji = {'positive': '😊', 'negative': '😞', 'neutral': '😐'}.get(sent, '❓')\n",
    "    print(f\"  {emoji} {sent}: {count} articles ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔤 4. Préprocessing du Texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🛠️ Fonction de préprocessing avancée\n",
    "def advanced_preprocess(text):\n",
    "    \"\"\"Préprocessing avancé du texte\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convertir en minuscules\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Supprimer les URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Supprimer les mentions et hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # Supprimer les chiffres\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Supprimer la ponctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Supprimer les espaces multiples\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Appliquer le préprocessing\n",
    "print(\"🔄 Application du préprocessing...\")\n",
    "df['processed_text'] = df['text'].apply(advanced_preprocess)\n",
    "\n",
    "# Calculer les nouvelles métriques\n",
    "df['processed_length'] = df['processed_text'].str.len()\n",
    "df['processed_words'] = df['processed_text'].str.split().str.len()\n",
    "\n",
    "print(\"✅ Préprocessing terminé!\")\n",
    "print(f\"📏 Réduction moyenne de longueur: {((df['text_length'] - df['processed_length']) / df['text_length'] * 100).mean():.1f}%\")\n",
    "\n",
    "# Exemples avant/après\n",
    "print(\"\\n📝 EXEMPLES DE TRANSFORMATION:\")\n",
    "print(\"=\" * 50)\n",
    "for i in range(min(3, len(df))):\n",
    "    print(f\"\\n🔸 Exemple {i+1}:\")\n",
    "    print(f\"   📄 Original: {df.iloc[i]['text'][:100]}...\")\n",
    "    print(f\"   🔧 Traité: {df.iloc[i]['processed_text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 5. Analyse de Fréquence des Mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 Analyse de fréquence des mots\n",
    "print(\"📊 ANALYSE DE FRÉQUENCE DES MOTS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Mots vides français et anglais courants\n",
    "stop_words = {\n",
    "    'le', 'de', 'et', 'à', 'un', 'il', 'être', 'et', 'en', 'avoir', 'que', 'pour',\n",
    "    'dans', 'ce', 'son', 'une', 'sur', 'avec', 'ne', 'se', 'pas', 'tout', 'plus',\n",
    "    'par', 'grand', 'en', 'être', 'et', 'en', 'avoir', 'que', 'pour', 'du', 'des',\n",
    "    'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',\n",
    "    'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n",
    "    'this', 'that', 'these', 'those', 'a', 'an', 'as', 'if', 'it', 'its'\n",
    "}\n",
    "\n",
    "# Extraire tous les mots\n",
    "all_words = []\n",
    "for text in df['processed_text']:\n",
    "    if isinstance(text, str):\n",
    "        words = text.split()\n",
    "        # Filtrer les mots courts et les mots vides\n",
    "        words = [word for word in words if len(word) > 2 and word not in stop_words]\n",
    "        all_words.extend(words)\n",
    "\n",
    "# Compter les fréquences\n",
    "word_freq = Counter(all_words)\n",
    "top_words = word_freq.most_common(20)\n",
    "\n",
    "print(f\"📝 Total de mots uniques: {len(word_freq)}\")\n",
    "print(f\"📊 Total d'occurrences: {sum(word_freq.values())}\")\n",
    "\n",
    "print(\"\\n🏆 TOP 20 DES MOTS LES PLUS FRÉQUENTS:\")\n",
    "for i, (word, freq) in enumerate(top_words, 1):\n",
    "    percentage = (freq / sum(word_freq.values())) * 100\n",
    "    print(f\"  {i:2d}. {word:<15} : {freq:3d} fois ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Visualisation de la fréquence des mots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Graphique en barres horizontales\n",
    "words, frequencies = zip(*top_words)\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(words)))\n",
    "\n",
    "bars = ax1.barh(range(len(words)), frequencies, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax1.set_yticks(range(len(words)))\n",
    "ax1.set_yticklabels(words)\n",
    "ax1.set_xlabel('Fréquence')\n",
    "ax1.set_title('🏆 Top 20 des Mots les Plus Fréquents', fontsize=14, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for i, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    ax1.text(width + 0.1, bar.get_y() + bar.get_height()/2,\n",
    "             f'{int(width)}', ha='left', va='center', fontweight='bold')\n",
    "\n",
    "# Distribution de Zipf (loi de puissance)\n",
    "ranks = np.arange(1, len(top_words) + 1)\n",
    "ax2.loglog(ranks, frequencies, 'bo-', alpha=0.7, linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Rang (log)')\n",
    "ax2.set_ylabel('Fréquence (log)')\n",
    "ax2.set_title('📈 Distribution de Zipf', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Ligne de tendance\n",
    "z = np.polyfit(np.log(ranks), np.log(frequencies), 1)\n",
    "p = np.poly1d(z)\n",
    "ax2.plot(ranks, np.exp(p(np.log(ranks))), \"r--\", alpha=0.8, linewidth=2, \n",
    "         label=f'Pente: {z[0]:.2f}')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"📊 Coefficient de Zipf: {abs(z[0]):.2f} (idéal ≈ 1.0)\")\n",
    "if abs(z[0]) > 0.8:\n",
    "    print(\"✅ Distribution proche de la loi de Zipf (texte naturel)\")\n",
    "else:\n",
    "    print(\"⚠️ Distribution éloignée de la loi de Zipf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ☁️ 6. Nuage de Mots et Visualisations Avancées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ☁️ Création de nuages de mots\n",
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "    \n",
    "    # Combiner tout le texte préprocessé\n",
    "    combined_text = ' '.join(df['processed_text'].dropna())\n",
    "    \n",
    "    # Créer le nuage de mots principal\n",
    "    wordcloud = WordCloud(\n",
    "        width=1200, \n",
    "        height=600, \n",
    "        background_color='white',\n",
    "        max_words=100,\n",
    "        colormap='viridis',\n",
    "        relative_scaling=0.5,\n",
    "        min_font_size=10\n",
    "    ).generate(combined_text)\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('☁️ Nuage de Mots Global', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Nuages de mots par sentiment\n",
    "    sentiments = df['sentiment'].unique()\n",
    "    if len(sentiments) > 1:\n",
    "        fig, axes = plt.subplots(1, len(sentiments), figsize=(5*len(sentiments), 6))\n",
    "        if len(sentiments) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        colors = {'positive': 'Greens', 'negative': 'Reds', 'neutral': 'Blues'}\n",
    "        \n",
    "        for i, sentiment in enumerate(sentiments):\n",
    "            sentiment_text = ' '.join(df[df['sentiment'] == sentiment]['processed_text'].dropna())\n",
    "            \n",
    "            if sentiment_text.strip():  # Vérifier que le texte n'est pas vide\n",
    "                wc = WordCloud(\n",
    "                    width=400, height=300,\n",
    "                    background_color='white',\n",
    "                    max_words=50,\n",
    "                    colormap=colors.get(sentiment, 'viridis')\n",
    "                ).generate(sentiment_text)\n",
    "                \n",
    "                axes[i].imshow(wc, interpolation='bilinear')\n",
    "                axes[i].axis('off')\n",
    "                emoji = {'positive': '😊', 'negative': '😞', 'neutral': '😐'}.get(sentiment, '❓')\n",
    "                axes[i].set_title(f'{emoji} {sentiment.title()}', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        plt.suptitle('☁️ Nuages de Mots par Sentiment', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"✅ Nuages de mots créés avec succès!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"⚠️ WordCloud non disponible. Affichage textuel des mots fréquents:\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    for word, freq in top_words[:15]:\n",
    "        bar_length = int((freq / max(frequencies)) * 40)\n",
    "        bar = \"█\" * bar_length\n",
    "        print(f\"{word:<15} {bar} {freq}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💾 7. Sauvegarde et Résumé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💾 Sauvegarde des données préprocessées\n",
    "import os\n",
    "\n",
    "# Créer le dossier processed s'il n'existe pas\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Sauvegarder le dataset enrichi\n",
    "output_file = '../data/processed/processed_news_data.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"💾 Données sauvegardées dans: {output_file}\")\n",
    "print(f\"📊 Taille du fichier: {os.path.getsize(output_file) / 1024:.2f} KB\")\n",
    "\n",
    "# Sauvegarder les statistiques de mots\n",
    "word_stats = pd.DataFrame(top_words, columns=['word', 'frequency'])\n",
    "word_stats['percentage'] = (word_stats['frequency'] / sum(word_freq.values())) * 100\n",
    "word_stats.to_csv('../data/processed/word_frequencies.csv', index=False)\n",
    "\n",
    "print(\"📝 Fréquences des mots sauvegardées dans: ../data/processed/word_frequencies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📋 Résumé final de l'analyse\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎉 RÉSUMÉ DE L'ANALYSE EXPLORATOIRE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n📊 DATASET:\")\n",
    "print(f\"   📏 {len(df)} articles analysés\")\n",
    "print(f\"   🏷️ {df['category'].nunique()} catégories: {', '.join(df['category'].unique())}\")\n",
    "print(f\"   😊 {df['sentiment'].nunique()} sentiments: {', '.join(df['sentiment'].unique())}\")\n",
    "\n",
    "print(f\"\\n📝 MÉTRIQUES DE TEXTE:\")\n",
    "print(f\"   📏 Longueur moyenne: {df['text_length'].mean():.1f} caractères\")\n",
    "print(f\"   🔤 Mots moyens: {df['word_count'].mean():.1f} mots\")\n",
    "print(f\"   📄 Phrases moyennes: {df['sentence_count'].mean():.1f} phrases\")\n",
    "print(f\"   🎯 Diversité lexicale: {df['lexical_diversity'].mean():.3f}\")\n",
    "\n",
    "print(f\"\\n🔍 ANALYSE LEXICALE:\")\n",
    "print(f\"   📚 {len(word_freq)} mots uniques identifiés\")\n",
    "print(f\"   🏆 Mot le plus fréquent: '{top_words[0][0]}' ({top_words[0][1]} fois)\")\n",
    "print(f\"   📊 Coefficient de Zipf: {abs(z[0]):.2f}\")\n",
    "\n",
    "print(f\"\\n✅ FICHIERS GÉNÉRÉS:\")\n",
    "print(f\"   💾 Dataset préprocessé: processed_news_data.csv\")\n",
    "print(f\"   📊 Fréquences des mots: word_frequencies.csv\")\n",
    "print(f\"   📈 Visualisations: Graphiques interactifs générés\")\n",
    "\n",
    "print(f\"\\n🚀 PROCHAINES ÉTAPES RECOMMANDÉES:\")\n",
    "print(f\"   1️⃣ Analyse de sentiment avec TextBlob/VADER\")\n",
    "print(f\"   2️⃣ Modélisation de sujets avec LDA/BERTopic\")\n",
    "print(f\"   3️⃣ Reconnaissance d'entités nommées (NER)\")\n",
    "print(f\"   4️⃣ Résumé automatique de texte\")\n",
    "print(f\"   5️⃣ Classification automatique\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎯 ANALYSE TERMINÉE AVEC SUCCÈS!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
