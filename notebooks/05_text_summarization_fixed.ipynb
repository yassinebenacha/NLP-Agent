{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“ RÃ©sumÃ© Automatique de Texte - Version Fonctionnelle\n",
    "\n",
    "Ce notebook implÃ©mente plusieurs techniques de rÃ©sumÃ© automatique pour extraire les informations essentielles des textes.\n",
    "\n",
    "## ğŸ¯ Objectifs:\n",
    "- ğŸ“Š RÃ©sumÃ© extractif avec TF-IDF et TextRank\n",
    "- ğŸ¤– RÃ©sumÃ© abstractif avec Transformers (si disponible)\n",
    "- ğŸ“ˆ Ã‰valuation avec mÃ©triques ROUGE\n",
    "- ğŸ” Comparaison des diffÃ©rentes approches\n",
    "- ğŸ’¾ Sauvegarde des rÃ©sumÃ©s gÃ©nÃ©rÃ©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… BibliothÃ¨ques de base importÃ©es!\n",
      "âœ… NLTK disponible!\n",
      "âœ… Scikit-learn disponible!\n",
      "ğŸš€ Transformers disponible!\n",
      "âœ… NetworkX disponible!\n",
      "ğŸ“ Dossiers crÃ©Ã©s!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“¦ Import des bibliothÃ¨ques essentielles\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import math\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration des graphiques\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print('âœ… BibliothÃ¨ques de base importÃ©es!')\n",
    "\n",
    "# Import NLTK pour la tokenisation\n",
    "nltk_available = False\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import PorterStemmer\n",
    "    \n",
    "    # TÃ©lÃ©charger les ressources nÃ©cessaires\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    \n",
    "    nltk_available = True\n",
    "    print('âœ… NLTK disponible!')\n",
    "except ImportError:\n",
    "    print('âš ï¸ NLTK non disponible - utilisation de mÃ©thodes alternatives')\n",
    "\n",
    "# Import scikit-learn pour TF-IDF\n",
    "sklearn_available = False\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    sklearn_available = True\n",
    "    print('âœ… Scikit-learn disponible!')\n",
    "except ImportError:\n",
    "    print('âš ï¸ Scikit-learn non disponible')\n",
    "\n",
    "# Import des bibliothÃ¨ques avancÃ©es (optionnelles)\n",
    "transformers_available = False\n",
    "try:\n",
    "    from transformers import pipeline\n",
    "    transformers_available = True\n",
    "    print('ğŸš€ Transformers disponible!')\n",
    "except ImportError:\n",
    "    print('âš ï¸ Transformers non disponible - rÃ©sumÃ© extractif uniquement')\n",
    "\n",
    "# NetworkX pour TextRank (optionnel)\n",
    "networkx_available = False\n",
    "try:\n",
    "    import networkx as nx\n",
    "    networkx_available = True\n",
    "    print('âœ… NetworkX disponible!')\n",
    "except ImportError:\n",
    "    print('âš ï¸ NetworkX non disponible - utilisation d\\'algorithmes alternatifs')\n",
    "\n",
    "# CrÃ©er les dossiers nÃ©cessaires\n",
    "os.makedirs('../visualizations', exist_ok=True)\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "print('ğŸ“ Dossiers crÃ©Ã©s!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‚ 1. Chargement et PrÃ©paration des DonnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DonnÃ©es chargÃ©es: (10, 9)\n",
      "ğŸ“Š Documents originaux: 10\n",
      "ğŸ“ Documents suffisamment longs: 10\n",
      "ğŸ“ Longueur moyenne: 385.2 caractÃ¨res\n",
      "\n",
      "ğŸ“‹ AperÃ§u des textes Ã  rÃ©sumer:\n",
      "\n",
      "ğŸ“„ Document 1 (461 caractÃ¨res):\n",
      "   A major technology company has announced a groundbreaking advancement in artificial intelligence that promises to transform how we interact with machi...\n",
      "\n",
      "ğŸ“„ Document 2 (412 caractÃ¨res):\n",
      "   World leaders have reached a historic agreement at the latest climate change summit, committing to ambitious targets for reducing greenhouse gas emiss...\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“Š Charger les donnÃ©es\n",
    "try:\n",
    "    df = pd.read_csv('../data/processed/processed_news_data.csv')\n",
    "    print(f\"âœ… DonnÃ©es chargÃ©es: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ Fichier processed_news_data.csv non trouvÃ©.\")\n",
    "    print(\"ğŸ”„ CrÃ©ation d'un dataset d'exemple avec textes longs...\")\n",
    "    \n",
    "    # Dataset d'exemple avec textes plus longs pour le rÃ©sumÃ©\n",
    "    sample_texts = [\n",
    "        \"\"\"Artificial intelligence has revolutionized the technology industry in unprecedented ways. Companies like OpenAI, Google, and Microsoft are investing billions of dollars in AI research and development. The recent breakthrough in large language models has enabled new applications in natural language processing, computer vision, and robotics. These advances are transforming industries from healthcare to finance, creating new opportunities while also raising important questions about ethics and job displacement. Experts predict that AI will continue to evolve rapidly, with potential applications in autonomous vehicles, personalized medicine, and climate change solutions. However, the development of AI also requires careful consideration of safety, privacy, and fairness to ensure that these powerful technologies benefit all of humanity.\"\"\",\n",
    "        \n",
    "        \"\"\"Climate change represents one of the most pressing challenges of our time, affecting ecosystems, weather patterns, and human societies worldwide. Rising global temperatures have led to melting ice caps, rising sea levels, and more frequent extreme weather events. Scientists from the Intergovernmental Panel on Climate Change have documented clear evidence of human activities contributing to greenhouse gas emissions. The transition to renewable energy sources such as solar, wind, and hydroelectric power is crucial for reducing carbon emissions. Many countries have committed to net-zero emissions targets by 2050, requiring significant changes in energy production, transportation, and industrial processes. Individual actions, such as reducing energy consumption and supporting sustainable practices, also play an important role in addressing this global challenge.\"\"\",\n",
    "        \n",
    "        \"\"\"The global economy has experienced significant volatility in recent years, influenced by factors such as the COVID-19 pandemic, geopolitical tensions, and supply chain disruptions. Central banks worldwide have implemented various monetary policies to stabilize markets and support economic recovery. Inflation rates have fluctuated, affecting consumer purchasing power and business investment decisions. The rise of digital currencies and fintech innovations has transformed traditional banking and payment systems. International trade relationships continue to evolve, with new agreements and partnerships reshaping global commerce. Economists emphasize the importance of sustainable economic growth that balances prosperity with environmental and social considerations for long-term stability.\"\"\",\n",
    "        \n",
    "        \"\"\"Healthcare systems around the world have undergone dramatic transformations, particularly in response to global health challenges. The development and distribution of vaccines have demonstrated the power of international scientific collaboration. Telemedicine and digital health technologies have expanded access to medical care, especially in remote and underserved communities. Precision medicine, powered by genomics and artificial intelligence, is enabling more personalized treatment approaches. Mental health awareness has increased significantly, leading to better support systems and reduced stigma. Healthcare professionals continue to advocate for preventive care and public health measures to improve population health outcomes while managing rising healthcare costs.\"\"\",\n",
    "        \n",
    "        \"\"\"Education has evolved rapidly with the integration of digital technologies and online learning platforms. The pandemic accelerated the adoption of remote learning, highlighting both opportunities and challenges in educational delivery. Students and educators have adapted to new tools and methodologies, from virtual classrooms to interactive learning applications. The concept of lifelong learning has become increasingly important as job markets evolve and new skills are required. Educational institutions are exploring innovative approaches such as competency-based learning, micro-credentials, and partnerships with industry. Access to quality education remains a global priority, with efforts to bridge digital divides and ensure equitable learning opportunities for all students.\"\"\"\n",
    "    ]\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'id': range(1, len(sample_texts) + 1),\n",
    "        'text': sample_texts,\n",
    "        'category': ['technology', 'environment', 'economy', 'health', 'education'],\n",
    "        'sentiment': ['positive', 'neutral', 'negative', 'positive', 'neutral']\n",
    "    })\n",
    "    print(f\"âœ… Dataset d'exemple crÃ©Ã©: {df.shape}\")\n",
    "\n",
    "# VÃ©rifier les colonnes nÃ©cessaires\n",
    "text_column = 'text'\n",
    "if text_column not in df.columns:\n",
    "    print(\"âŒ Colonne 'text' non trouvÃ©e\")\n",
    "    exit()\n",
    "\n",
    "# Filtrer les textes suffisamment longs pour le rÃ©sumÃ©\n",
    "min_length = 200  # Minimum 200 caractÃ¨res\n",
    "df_long = df[df[text_column].str.len() >= min_length].copy()\n",
    "\n",
    "print(f\"ğŸ“Š Documents originaux: {len(df)}\")\n",
    "print(f\"ğŸ“ Documents suffisamment longs: {len(df_long)}\")\n",
    "print(f\"ğŸ“ Longueur moyenne: {df_long[text_column].str.len().mean():.1f} caractÃ¨res\")\n",
    "\n",
    "if len(df_long) == 0:\n",
    "    print(\"âš ï¸ Aucun document suffisamment long pour le rÃ©sumÃ©\")\n",
    "    df_long = df.copy()  # Utiliser tous les documents\n",
    "\n",
    "# AperÃ§u des donnÃ©es\n",
    "print(\"\\nğŸ“‹ AperÃ§u des textes Ã  rÃ©sumer:\")\n",
    "for i in range(min(2, len(df_long))):\n",
    "    text = df_long[text_column].iloc[i]\n",
    "    print(f\"\\nğŸ“„ Document {i+1} ({len(text)} caractÃ¨res):\")\n",
    "    print(f\"   {text[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ 2. Fonctions Utilitaires pour le RÃ©sumÃ©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fonctions utilitaires dÃ©finies!\n",
      "ğŸ“ NLTK disponible: True\n",
      "ğŸ”¤ Nombre de mots vides: 198\n"
     ]
    }
   ],
   "source": [
    "# ğŸ› ï¸ Fonctions utilitaires pour le traitement de texte\n",
    "\n",
    "def simple_sentence_tokenize(text):\n",
    "    \"\"\"Tokenisation simple des phrases si NLTK n'est pas disponible\"\"\"\n",
    "    if nltk_available:\n",
    "        return sent_tokenize(text)\n",
    "    else:\n",
    "        # MÃ©thode simple basÃ©e sur la ponctuation\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "def simple_word_tokenize(text):\n",
    "    \"\"\"Tokenisation simple des mots\"\"\"\n",
    "    if nltk_available:\n",
    "        return word_tokenize(text.lower())\n",
    "    else:\n",
    "        # MÃ©thode simple\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        return words\n",
    "\n",
    "def get_stop_words():\n",
    "    \"\"\"Obtenir la liste des mots vides\"\"\"\n",
    "    if nltk_available:\n",
    "        try:\n",
    "            return set(stopwords.words('english'))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Liste de mots vides de base\n",
    "    return {\n",
    "        'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',\n",
    "        'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n",
    "        'this', 'that', 'these', 'those', 'a', 'an', 'as', 'if', 'it', 'its',\n",
    "        'will', 'would', 'could', 'should', 'may', 'might', 'can', 'must'\n",
    "    }\n",
    "\n",
    "def calculate_word_frequencies(text):\n",
    "    \"\"\"Calculer les frÃ©quences des mots\"\"\"\n",
    "    words = simple_word_tokenize(text)\n",
    "    stop_words = get_stop_words()\n",
    "    \n",
    "    # Filtrer les mots vides et courts\n",
    "    filtered_words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    # Compter les frÃ©quences\n",
    "    word_freq = Counter(filtered_words)\n",
    "    \n",
    "    # Normaliser les frÃ©quences\n",
    "    max_freq = max(word_freq.values()) if word_freq else 1\n",
    "    for word in word_freq:\n",
    "        word_freq[word] = word_freq[word] / max_freq\n",
    "    \n",
    "    return word_freq\n",
    "\n",
    "def score_sentences(sentences, word_freq):\n",
    "    \"\"\"Scorer les phrases basÃ© sur les frÃ©quences des mots\"\"\"\n",
    "    sentence_scores = {}\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = simple_word_tokenize(sentence)\n",
    "        score = 0\n",
    "        word_count = 0\n",
    "        \n",
    "        for word in words:\n",
    "            if word in word_freq:\n",
    "                score += word_freq[word]\n",
    "                word_count += 1\n",
    "        \n",
    "        if word_count > 0:\n",
    "            sentence_scores[sentence] = score / word_count\n",
    "        else:\n",
    "            sentence_scores[sentence] = 0\n",
    "    \n",
    "    return sentence_scores\n",
    "\n",
    "print(\"âœ… Fonctions utilitaires dÃ©finies!\")\n",
    "print(f\"ğŸ“ NLTK disponible: {nltk_available}\")\n",
    "print(f\"ğŸ”¤ Nombre de mots vides: {len(get_stop_words())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š 3. RÃ©sumÃ© Extractif avec FrÃ©quences TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ RÃ‰SUMÃ‰ EXTRACTIF\n",
      "========================================\n",
      "ğŸ”„ GÃ©nÃ©ration des rÃ©sumÃ©s extractifs...\n",
      "âœ… RÃ©sumÃ©s extractifs gÃ©nÃ©rÃ©s!\n",
      "\n",
      "ğŸ“Š STATISTIQUES DE COMPRESSION:\n",
      "  ğŸ“ Longueur originale moyenne: 385 caractÃ¨res\n",
      "  ğŸ”¤ RÃ©sumÃ© frÃ©quence moyenne: 385 caractÃ¨res\n",
      "  ğŸ“Š RÃ©sumÃ© TF-IDF moyenne: 385 caractÃ¨res\n",
      "  ğŸ“‰ Compression frÃ©quence: 0.0%\n",
      "  ğŸ“‰ Compression TF-IDF: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ RÃ©sumÃ© extractif basÃ© sur les frÃ©quences des mots\n",
    "def frequency_based_summary(text, num_sentences=3):\n",
    "    \"\"\"RÃ©sumÃ© extractif basÃ© sur les frÃ©quences des mots\"\"\"\n",
    "    sentences = simple_sentence_tokenize(text)\n",
    "    \n",
    "    if len(sentences) <= num_sentences:\n",
    "        return text\n",
    "    \n",
    "    # Calculer les frÃ©quences des mots\n",
    "    word_freq = calculate_word_frequencies(text)\n",
    "    \n",
    "    # Scorer les phrases\n",
    "    sentence_scores = score_sentences(sentences, word_freq)\n",
    "    \n",
    "    # SÃ©lectionner les meilleures phrases\n",
    "    best_sentences = sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)[:num_sentences]\n",
    "    \n",
    "    # RÃ©ordonner selon l'ordre original\n",
    "    summary_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if any(sentence == best[0] for best in best_sentences):\n",
    "            summary_sentences.append(sentence)\n",
    "    \n",
    "    return ' '.join(summary_sentences)\n",
    "\n",
    "# RÃ©sumÃ© avec TF-IDF si scikit-learn est disponible\n",
    "def tfidf_based_summary(text, num_sentences=3):\n",
    "    \"\"\"RÃ©sumÃ© extractif basÃ© sur TF-IDF\"\"\"\n",
    "    if not sklearn_available:\n",
    "        return frequency_based_summary(text, num_sentences)\n",
    "    \n",
    "    sentences = simple_sentence_tokenize(text)\n",
    "    \n",
    "    if len(sentences) <= num_sentences:\n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        # Vectorisation TF-IDF\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', max_features=100)\n",
    "        tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "        \n",
    "        # Calculer les scores des phrases (somme des scores TF-IDF)\n",
    "        sentence_scores = tfidf_matrix.sum(axis=1).A1\n",
    "        \n",
    "        # SÃ©lectionner les meilleures phrases\n",
    "        top_indices = sentence_scores.argsort()[-num_sentences:][::-1]\n",
    "        \n",
    "        # RÃ©ordonner selon l'ordre original\n",
    "        summary_sentences = [sentences[i] for i in sorted(top_indices)]\n",
    "        \n",
    "        return ' '.join(summary_sentences)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Erreur TF-IDF: {e}\")\n",
    "        return frequency_based_summary(text, num_sentences)\n",
    "\n",
    "print(\"ğŸ¯ RÃ‰SUMÃ‰ EXTRACTIF\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Appliquer le rÃ©sumÃ© extractif\n",
    "print(\"ğŸ”„ GÃ©nÃ©ration des rÃ©sumÃ©s extractifs...\")\n",
    "\n",
    "df_long['frequency_summary'] = df_long[text_column].apply(\n",
    "    lambda x: frequency_based_summary(x, num_sentences=3)\n",
    ")\n",
    "\n",
    "df_long['tfidf_summary'] = df_long[text_column].apply(\n",
    "    lambda x: tfidf_based_summary(x, num_sentences=3)\n",
    ")\n",
    "\n",
    "print(\"âœ… RÃ©sumÃ©s extractifs gÃ©nÃ©rÃ©s!\")\n",
    "\n",
    "# Calculer les statistiques de compression\n",
    "original_lengths = df_long[text_column].str.len()\n",
    "freq_summary_lengths = df_long['frequency_summary'].str.len()\n",
    "tfidf_summary_lengths = df_long['tfidf_summary'].str.len()\n",
    "\n",
    "freq_compression = (1 - freq_summary_lengths / original_lengths) * 100\n",
    "tfidf_compression = (1 - tfidf_summary_lengths / original_lengths) * 100\n",
    "\n",
    "print(f\"\\nğŸ“Š STATISTIQUES DE COMPRESSION:\")\n",
    "print(f\"  ğŸ“ Longueur originale moyenne: {original_lengths.mean():.0f} caractÃ¨res\")\n",
    "print(f\"  ğŸ”¤ RÃ©sumÃ© frÃ©quence moyenne: {freq_summary_lengths.mean():.0f} caractÃ¨res\")\n",
    "print(f\"  ğŸ“Š RÃ©sumÃ© TF-IDF moyenne: {tfidf_summary_lengths.mean():.0f} caractÃ¨res\")\n",
    "print(f\"  ğŸ“‰ Compression frÃ©quence: {freq_compression.mean():.1f}%\")\n",
    "print(f\"  ğŸ“‰ Compression TF-IDF: {tfidf_compression.mean():.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
