{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìù R√©sum√© Automatique de Texte - Version Fonctionnelle\n",
    "\n",
    "Ce notebook impl√©mente plusieurs techniques de r√©sum√© automatique pour extraire les informations essentielles des textes.\n",
    "\n",
    "## üéØ Objectifs:\n",
    "- üìä R√©sum√© extractif avec TF-IDF et TextRank\n",
    "- ü§ñ R√©sum√© abstractif avec Transformers (si disponible)\n",
    "- üìà √âvaluation avec m√©triques ROUGE\n",
    "- üîç Comparaison des diff√©rentes approches\n",
    "- üíæ Sauvegarde des r√©sum√©s g√©n√©r√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Biblioth√®ques de base import√©es!\n",
      "‚úÖ NLTK disponible!\n",
      "‚úÖ Scikit-learn disponible!\n",
      "üöÄ Transformers disponible!\n",
      "‚úÖ NetworkX disponible!\n",
      "üìÅ Dossiers cr√©√©s!\n"
     ]
    }
   ],
   "source": [
    "# üì¶ Import des biblioth√®ques essentielles\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import math\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration des graphiques\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print('‚úÖ Biblioth√®ques de base import√©es!')\n",
    "\n",
    "# Import NLTK pour la tokenisation\n",
    "nltk_available = False\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import PorterStemmer\n",
    "    \n",
    "    # T√©l√©charger les ressources n√©cessaires\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    \n",
    "    nltk_available = True\n",
    "    print('‚úÖ NLTK disponible!')\n",
    "except ImportError:\n",
    "    print('‚ö†Ô∏è NLTK non disponible - utilisation de m√©thodes alternatives')\n",
    "\n",
    "# Import scikit-learn pour TF-IDF\n",
    "sklearn_available = False\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    sklearn_available = True\n",
    "    print('‚úÖ Scikit-learn disponible!')\n",
    "except ImportError:\n",
    "    print('‚ö†Ô∏è Scikit-learn non disponible')\n",
    "\n",
    "# Import des biblioth√®ques avanc√©es (optionnelles)\n",
    "transformers_available = False\n",
    "try:\n",
    "    from transformers import pipeline\n",
    "    transformers_available = True\n",
    "    print('üöÄ Transformers disponible!')\n",
    "except ImportError:\n",
    "    print('‚ö†Ô∏è Transformers non disponible - r√©sum√© extractif uniquement')\n",
    "\n",
    "# NetworkX pour TextRank (optionnel)\n",
    "networkx_available = False\n",
    "try:\n",
    "    import networkx as nx\n",
    "    networkx_available = True\n",
    "    print('‚úÖ NetworkX disponible!')\n",
    "except ImportError:\n",
    "    print('‚ö†Ô∏è NetworkX non disponible - utilisation d\\'algorithmes alternatifs')\n",
    "\n",
    "# Cr√©er les dossiers n√©cessaires\n",
    "os.makedirs('../visualizations', exist_ok=True)\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "print('üìÅ Dossiers cr√©√©s!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ 1. Chargement et Pr√©paration des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Donn√©es charg√©es: (10, 9)\n",
      "üìä Documents originaux: 10\n",
      "üìù Documents suffisamment longs: 10\n",
      "üìè Longueur moyenne: 385.2 caract√®res\n",
      "\n",
      "üìã Aper√ßu des textes √† r√©sumer:\n",
      "\n",
      "üìÑ Document 1 (461 caract√®res):\n",
      "   A major technology company has announced a groundbreaking advancement in artificial intelligence that promises to transform how we interact with machi...\n",
      "\n",
      "üìÑ Document 2 (412 caract√®res):\n",
      "   World leaders have reached a historic agreement at the latest climate change summit, committing to ambitious targets for reducing greenhouse gas emiss...\n"
     ]
    }
   ],
   "source": [
    "# üìä Charger les donn√©es\n",
    "try:\n",
    "    df = pd.read_csv('../data/processed/processed_news_data.csv')\n",
    "    print(f\"‚úÖ Donn√©es charg√©es: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Fichier processed_news_data.csv non trouv√©.\")\n",
    "    print(\"üîÑ Cr√©ation d'un dataset d'exemple avec textes longs...\")\n",
    "    \n",
    "    # Dataset d'exemple avec textes plus longs pour le r√©sum√©\n",
    "    sample_texts = [\n",
    "        \"\"\"Artificial intelligence has revolutionized the technology industry in unprecedented ways. Companies like OpenAI, Google, and Microsoft are investing billions of dollars in AI research and development. The recent breakthrough in large language models has enabled new applications in natural language processing, computer vision, and robotics. These advances are transforming industries from healthcare to finance, creating new opportunities while also raising important questions about ethics and job displacement. Experts predict that AI will continue to evolve rapidly, with potential applications in autonomous vehicles, personalized medicine, and climate change solutions. However, the development of AI also requires careful consideration of safety, privacy, and fairness to ensure that these powerful technologies benefit all of humanity.\"\"\",\n",
    "        \n",
    "        \"\"\"Climate change represents one of the most pressing challenges of our time, affecting ecosystems, weather patterns, and human societies worldwide. Rising global temperatures have led to melting ice caps, rising sea levels, and more frequent extreme weather events. Scientists from the Intergovernmental Panel on Climate Change have documented clear evidence of human activities contributing to greenhouse gas emissions. The transition to renewable energy sources such as solar, wind, and hydroelectric power is crucial for reducing carbon emissions. Many countries have committed to net-zero emissions targets by 2050, requiring significant changes in energy production, transportation, and industrial processes. Individual actions, such as reducing energy consumption and supporting sustainable practices, also play an important role in addressing this global challenge.\"\"\",\n",
    "        \n",
    "        \"\"\"The global economy has experienced significant volatility in recent years, influenced by factors such as the COVID-19 pandemic, geopolitical tensions, and supply chain disruptions. Central banks worldwide have implemented various monetary policies to stabilize markets and support economic recovery. Inflation rates have fluctuated, affecting consumer purchasing power and business investment decisions. The rise of digital currencies and fintech innovations has transformed traditional banking and payment systems. International trade relationships continue to evolve, with new agreements and partnerships reshaping global commerce. Economists emphasize the importance of sustainable economic growth that balances prosperity with environmental and social considerations for long-term stability.\"\"\",\n",
    "        \n",
    "        \"\"\"Healthcare systems around the world have undergone dramatic transformations, particularly in response to global health challenges. The development and distribution of vaccines have demonstrated the power of international scientific collaboration. Telemedicine and digital health technologies have expanded access to medical care, especially in remote and underserved communities. Precision medicine, powered by genomics and artificial intelligence, is enabling more personalized treatment approaches. Mental health awareness has increased significantly, leading to better support systems and reduced stigma. Healthcare professionals continue to advocate for preventive care and public health measures to improve population health outcomes while managing rising healthcare costs.\"\"\",\n",
    "        \n",
    "        \"\"\"Education has evolved rapidly with the integration of digital technologies and online learning platforms. The pandemic accelerated the adoption of remote learning, highlighting both opportunities and challenges in educational delivery. Students and educators have adapted to new tools and methodologies, from virtual classrooms to interactive learning applications. The concept of lifelong learning has become increasingly important as job markets evolve and new skills are required. Educational institutions are exploring innovative approaches such as competency-based learning, micro-credentials, and partnerships with industry. Access to quality education remains a global priority, with efforts to bridge digital divides and ensure equitable learning opportunities for all students.\"\"\"\n",
    "    ]\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'id': range(1, len(sample_texts) + 1),\n",
    "        'text': sample_texts,\n",
    "        'category': ['technology', 'environment', 'economy', 'health', 'education'],\n",
    "        'sentiment': ['positive', 'neutral', 'negative', 'positive', 'neutral']\n",
    "    })\n",
    "    print(f\"‚úÖ Dataset d'exemple cr√©√©: {df.shape}\")\n",
    "\n",
    "# V√©rifier les colonnes n√©cessaires\n",
    "text_column = 'text'\n",
    "if text_column not in df.columns:\n",
    "    print(\"‚ùå Colonne 'text' non trouv√©e\")\n",
    "    exit()\n",
    "\n",
    "# Filtrer les textes suffisamment longs pour le r√©sum√©\n",
    "min_length = 200  # Minimum 200 caract√®res\n",
    "df_long = df[df[text_column].str.len() >= min_length].copy()\n",
    "\n",
    "print(f\"üìä Documents originaux: {len(df)}\")\n",
    "print(f\"üìù Documents suffisamment longs: {len(df_long)}\")\n",
    "print(f\"üìè Longueur moyenne: {df_long[text_column].str.len().mean():.1f} caract√®res\")\n",
    "\n",
    "if len(df_long) == 0:\n",
    "    print(\"‚ö†Ô∏è Aucun document suffisamment long pour le r√©sum√©\")\n",
    "    df_long = df.copy()  # Utiliser tous les documents\n",
    "\n",
    "# Aper√ßu des donn√©es\n",
    "print(\"\\nüìã Aper√ßu des textes √† r√©sumer:\")\n",
    "for i in range(min(2, len(df_long))):\n",
    "    text = df_long[text_column].iloc[i]\n",
    "    print(f\"\\nüìÑ Document {i+1} ({len(text)} caract√®res):\")\n",
    "    print(f\"   {text[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß 2. Fonctions Utilitaires pour le R√©sum√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fonctions utilitaires d√©finies!\n",
      "üìù NLTK disponible: True\n",
      "üî§ Nombre de mots vides: 198\n"
     ]
    }
   ],
   "source": [
    "# üõ†Ô∏è Fonctions utilitaires pour le traitement de texte\n",
    "\n",
    "def simple_sentence_tokenize(text):\n",
    "    \"\"\"Tokenisation simple des phrases si NLTK n'est pas disponible\"\"\"\n",
    "    if nltk_available:\n",
    "        return sent_tokenize(text)\n",
    "    else:\n",
    "        # M√©thode simple bas√©e sur la ponctuation\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "def simple_word_tokenize(text):\n",
    "    \"\"\"Tokenisation simple des mots\"\"\"\n",
    "    if nltk_available:\n",
    "        return word_tokenize(text.lower())\n",
    "    else:\n",
    "        # M√©thode simple\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        return words\n",
    "\n",
    "def get_stop_words():\n",
    "    \"\"\"Obtenir la liste des mots vides\"\"\"\n",
    "    if nltk_available:\n",
    "        try:\n",
    "            return set(stopwords.words('english'))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Liste de mots vides de base\n",
    "    return {\n",
    "        'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',\n",
    "        'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n",
    "        'this', 'that', 'these', 'those', 'a', 'an', 'as', 'if', 'it', 'its',\n",
    "        'will', 'would', 'could', 'should', 'may', 'might', 'can', 'must'\n",
    "    }\n",
    "\n",
    "def calculate_word_frequencies(text):\n",
    "    \"\"\"Calculer les fr√©quences des mots\"\"\"\n",
    "    words = simple_word_tokenize(text)\n",
    "    stop_words = get_stop_words()\n",
    "    \n",
    "    # Filtrer les mots vides et courts\n",
    "    filtered_words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    # Compter les fr√©quences\n",
    "    word_freq = Counter(filtered_words)\n",
    "    \n",
    "    # Normaliser les fr√©quences\n",
    "    max_freq = max(word_freq.values()) if word_freq else 1\n",
    "    for word in word_freq:\n",
    "        word_freq[word] = word_freq[word] / max_freq\n",
    "    \n",
    "    return word_freq\n",
    "\n",
    "def score_sentences(sentences, word_freq):\n",
    "    \"\"\"Scorer les phrases bas√© sur les fr√©quences des mots\"\"\"\n",
    "    sentence_scores = {}\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = simple_word_tokenize(sentence)\n",
    "        score = 0\n",
    "        word_count = 0\n",
    "        \n",
    "        for word in words:\n",
    "            if word in word_freq:\n",
    "                score += word_freq[word]\n",
    "                word_count += 1\n",
    "        \n",
    "        if word_count > 0:\n",
    "            sentence_scores[sentence] = score / word_count\n",
    "        else:\n",
    "            sentence_scores[sentence] = 0\n",
    "    \n",
    "    return sentence_scores\n",
    "\n",
    "print(\"‚úÖ Fonctions utilitaires d√©finies!\")\n",
    "print(f\"üìù NLTK disponible: {nltk_available}\")\n",
    "print(f\"üî§ Nombre de mots vides: {len(get_stop_words())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 3. R√©sum√© Extractif avec Fr√©quences TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ R√âSUM√â EXTRACTIF\n",
      "========================================\n",
      "üîÑ G√©n√©ration des r√©sum√©s extractifs...\n",
      "‚úÖ R√©sum√©s extractifs g√©n√©r√©s!\n",
      "\n",
      "üìä STATISTIQUES DE COMPRESSION:\n",
      "  üìè Longueur originale moyenne: 385 caract√®res\n",
      "  üî§ R√©sum√© fr√©quence moyenne: 385 caract√®res\n",
      "  üìä R√©sum√© TF-IDF moyenne: 385 caract√®res\n",
      "  üìâ Compression fr√©quence: 0.0%\n",
      "  üìâ Compression TF-IDF: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# üéØ R√©sum√© extractif bas√© sur les fr√©quences des mots\n",
    "def frequency_based_summary(text, num_sentences=3):\n",
    "    \"\"\"R√©sum√© extractif bas√© sur les fr√©quences des mots\"\"\"\n",
    "    sentences = simple_sentence_tokenize(text)\n",
    "    \n",
    "    if len(sentences) <= num_sentences:\n",
    "        return text\n",
    "    \n",
    "    # Calculer les fr√©quences des mots\n",
    "    word_freq = calculate_word_frequencies(text)\n",
    "    \n",
    "    # Scorer les phrases\n",
    "    sentence_scores = score_sentences(sentences, word_freq)\n",
    "    \n",
    "    # S√©lectionner les meilleures phrases\n",
    "    best_sentences = sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)[:num_sentences]\n",
    "    \n",
    "    # R√©ordonner selon l'ordre original\n",
    "    summary_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if any(sentence == best[0] for best in best_sentences):\n",
    "            summary_sentences.append(sentence)\n",
    "    \n",
    "    return ' '.join(summary_sentences)\n",
    "\n",
    "# R√©sum√© avec TF-IDF si scikit-learn est disponible\n",
    "def tfidf_based_summary(text, num_sentences=3):\n",
    "    \"\"\"R√©sum√© extractif bas√© sur TF-IDF\"\"\"\n",
    "    if not sklearn_available:\n",
    "        return frequency_based_summary(text, num_sentences)\n",
    "    \n",
    "    sentences = simple_sentence_tokenize(text)\n",
    "    \n",
    "    if len(sentences) <= num_sentences:\n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        # Vectorisation TF-IDF\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', max_features=100)\n",
    "        tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "        \n",
    "        # Calculer les scores des phrases (somme des scores TF-IDF)\n",
    "        sentence_scores = tfidf_matrix.sum(axis=1).A1\n",
    "        \n",
    "        # S√©lectionner les meilleures phrases\n",
    "        top_indices = sentence_scores.argsort()[-num_sentences:][::-1]\n",
    "        \n",
    "        # R√©ordonner selon l'ordre original\n",
    "        summary_sentences = [sentences[i] for i in sorted(top_indices)]\n",
    "        \n",
    "        return ' '.join(summary_sentences)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur TF-IDF: {e}\")\n",
    "        return frequency_based_summary(text, num_sentences)\n",
    "\n",
    "print(\"üéØ R√âSUM√â EXTRACTIF\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Appliquer le r√©sum√© extractif\n",
    "print(\"üîÑ G√©n√©ration des r√©sum√©s extractifs...\")\n",
    "\n",
    "df_long['frequency_summary'] = df_long[text_column].apply(\n",
    "    lambda x: frequency_based_summary(x, num_sentences=3)\n",
    ")\n",
    "\n",
    "df_long['tfidf_summary'] = df_long[text_column].apply(\n",
    "    lambda x: tfidf_based_summary(x, num_sentences=3)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ R√©sum√©s extractifs g√©n√©r√©s!\")\n",
    "\n",
    "# Calculer les statistiques de compression\n",
    "original_lengths = df_long[text_column].str.len()\n",
    "freq_summary_lengths = df_long['frequency_summary'].str.len()\n",
    "tfidf_summary_lengths = df_long['tfidf_summary'].str.len()\n",
    "\n",
    "freq_compression = (1 - freq_summary_lengths / original_lengths) * 100\n",
    "tfidf_compression = (1 - tfidf_summary_lengths / original_lengths) * 100\n",
    "\n",
    "print(f\"\\nüìä STATISTIQUES DE COMPRESSION:\")\n",
    "print(f\"  üìè Longueur originale moyenne: {original_lengths.mean():.0f} caract√®res\")\n",
    "print(f\"  üî§ R√©sum√© fr√©quence moyenne: {freq_summary_lengths.mean():.0f} caract√®res\")\n",
    "print(f\"  üìä R√©sum√© TF-IDF moyenne: {tfidf_summary_lengths.mean():.0f} caract√®res\")\n",
    "print(f\"  üìâ Compression fr√©quence: {freq_compression.mean():.1f}%\")\n",
    "print(f\"  üìâ Compression TF-IDF: {tfidf_compression.mean():.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
